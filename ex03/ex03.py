# -*- coding: utf-8 -*-
"""ex03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YGq6bl__HXwi0bsJtRL0XhUxSXMsoRSI
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
#from tqdm import tqdm_notebook
from argparse import Namespace
import os

"""This part with Google Drive not useful any more, was kind of a shitty solution anyway. Upload the file each time (little arrow on the right =) ) --> obvsl this info is for me as reminder, David, not for you^^"""

#from google.colab import drive
#drive.mount('/content/drive')

# Constants

# File paths
#TWEETS_FP = "tweets.json"
TWEETS_FP = 'tweets.json'
TRAIN_DEV_FP = 'labels-train+dev.tsv'
TEST_FP = 'labels-test.tsv'

# Column names
COL_ID = 'ID'
COL_TWEET = 'Tweet'
COL_LABEL = 'Label'

# minimum number of instances that we require to be present in the training set
# for a given language to be included in fitting of the model
MIN_NR_OCCURENCES = 1000

# unknown class name
CLASS_UNK = 'unknown'

"""This code down there is basically a copy of the function get_tweets(), it was simply to test all preprocessing steps and have a result on the first ten lines

So far this class Vocabulary is just taken from the ex02, it's nothing special yet, but it is probably that no (big) changes will  be needed for it...
"""

class Vocabulary:
    def __init__(self, add_unk=True):
        self._token_to_ids = {}
        self._ids_to_token = {}

        # Add the unknown token and index
        if add_unk:
            self.unk_index = self.add_token("<UNK>")
        else:
            self.unk_index = -1

    def vocabulary_set(self):
        """this function returns a list of unique tokens"""
        return self._ids_to_token.values()

    def add_token(self, token):
        """Update mapping dicts based on the token.

        Args:
            token (str): the item to add into the Vocabulary
        Returns:
            index (int): the integer corresponding to the token
        """
        if token in self._token_to_ids:
            index = self._token_to_ids[token]
        else:
            index = len(self._token_to_ids)
            self._token_to_ids[token] = index
            self._ids_to_token[index] = token
        return index

    def lookup_token(self, token):
        """Retrieve the index associated with the token
          or the UNK index if token isn't present.

        Args:
            token (str): the token to look up
        Returns:
            index (int): the index corresponding to the token
        Notes:
            `unk_index` needs to be >=0 (having been added into the Vocabulary)
              for the UNK functionality
        """
        if self.unk_index >= 0:
            return self._token_to_ids.get(token, self.unk_index)
        else:
            return self._token_to_ids[token]

    def lookup_index(self, index):
        """Return the token associated with the index

        Args:
            index (int): the index to look up
        Returns:
            token (str): the token corresponding to the index
        Raises:
            KeyError: if the index is not in the Vocabulary
        """
        if index not in self._ids_to_token:
            raise KeyError("the index (%d) is not in the Vocabulary" % index)
        return self._ids_to_token[index]

    def __len__(self):
        return len(self._token_to_ids)

class TweetsDataset(Dataset):
  def __init__(self, dataframes):
    """
    Args : I don't really know yet, this init is not ready as of yet
    """
    self.train_df = dataframes[0]
    self.dev_df = dataframes[1]
    self.test_df = dataframes[2]

    #otherwise possible:
    self.X_train = self.train_df.Tweet 
    self.y_train = self.train_df.Label
    self.X_dev = self.dev_df.Tweet
    self.y_dev = self.dev_df.Label
    self.X_test = self.test_df.Tweet
    self.y_test = self.test_df.Label

    self._lookup_dict = {'train' : self.train_df, 
                         'val': self.dev_df, 
                         'test': self.test_df}

    #self._vectorizer = Vectorizer.from_dataframe(self.train_df)
    self.set_split()

    self._vectorizer = vectorizer

    #class weight --> needed?
    #self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)


  @classmethod
  def load_and_create_dataset(cls):
    """
    Args: filepath
    """
    tweets = TweetsDataset._get_tweets()
    train_labels = TweetsDataset._get_train_labels()
    test_labels = TweetsDataset._get_test_labels()
    data = TweetsDataset._create_sets(tweets, train_labels, test_labels, use_dev=False)
    return cls(data)
  
  def _create_sets(tweets, train_dev_labels, test_labels, use_dev=True):
    """Return a tuple of dataframes comprising three main data sets"""

    # to allow for merge, need the same type
    tweets[COL_ID] = tweets[COL_ID].astype(int)

    # Merge by ID
    train_dev_data = pd.merge(tweets, train_dev_labels, on=COL_ID)
    test_data = pd.merge(tweets, test_labels, on=COL_ID)

    # Util function
    def drop_n_shuffle(data):
        data_no_na = data.dropna().copy()
        return data_no_na.sample(frac=1)

    frac = 1
    if use_dev:
        frac = 0.9

    train_dev_data_prepared = drop_n_shuffle(
        train_dev_data
    ).reset_index(drop=True)
    # take 90% of the data, reshuffle
    train_set = train_dev_data_prepared.sample(frac=frac, random_state=0)
    # take 10% that remain
    dev_set = train_dev_data_prepared.drop(train_set.index)
    test_set = drop_n_shuffle(test_data)

    # drop the ID columns, not needed anymore
    train = train_set.drop(COL_ID, axis=1)
    dev = dev_set.drop(COL_ID, axis=1)
    test = test_set.drop(COL_ID, axis=1)

    return train, dev, test
  
  def _get_train_labels():
    """Return a dataframe of train_dev labels"""
    train_dev_labels = pd.read_csv(
        'labels-train+dev.tsv',
        sep='\t',
        header=None,
        names=[COL_LABEL, COL_ID]
    )
    # remove whitespace from labels (e.g. "ar  " should be equal to "ar")
    train_dev_labels.Label = train_dev_labels.Label.str.strip()

    # deal with class imbalance in the train set
    lang_occurence = train_dev_labels.groupby(COL_LABEL).size()
    balanced_languages = lang_occurence.where(
        lang_occurence >= 1000  #minimum number of occurrences is 1000
    ).dropna().index.values
    balanced_labels = train_dev_labels.Label.isin(balanced_languages)


    # Option 1 - replace rows that are labelled with an imbalanced language
    # ~ is element-wise logical not
    train_dev_labels.loc[~balanced_labels, COL_LABEL] = CLASS_UNK

    # Option 2 - keep the rows that are labelled with a balanced language
    # train_dev_labels = train_dev_labels[balanced_labels]
    return train_dev_labels

  def _get_test_labels():
    """Return a dataframe of test labels"""
    return pd.read_csv(
        'labels-test.tsv',
        sep='\t',
        header=None,
        names=[COL_LABEL, COL_ID]
    )
  
  
  def _get_tweets():
    """Return a dataframe of tweets"""
    tweets = []
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags=re.UNICODE)
    
    with open('tweets.json', 'r') as tweets_fh:  # Tweets file handle
        for line in tweets_fh:   # put each line in a list of lines
            j_content = json.loads(line)
            #preprocessing steps first!
            text = j_content[1]
            text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
            text = re.sub('@\S*', '', text)
            j_content[1] = text
            tweets.append(j_content)

    # make a dataframe out of it
    tweets = pd.DataFrame(tweets, columns=[COL_ID, COL_TWEET])
    return tweets
  
  def get_vectorizer(self):
    """returns the vectorizer"""
    return self._vectorizer
  
  def set_split(self, split = "train"):
    """selects the splits in the dataset"""
    self._target_df = self._lookup_dict[split]
  
  def __len__(self):
    return len(self.train_df)+len(self.dev_df)+len(self.test_df)
  
  #from that point on, need help please
  def __getitem__(self, index):
    """WHAT?"""
    pass

class Vectorizer(object):
    """ The Vectorizer which coordinates the Vocabularies and puts them to use"""
    def __init__(self, token_vocab, label_vocab, max_tweet_length = 256):
        """
        Args:
            surname_vocab (Vocabulary): maps characters to integers
            nationality_vocab (Vocabulary): maps nationalities to integers
            max_surname_length (int): the length of the longest surname
        """
        self.token_vocab = token_vocab
        self.label_vocab = label_vocab
        self._max_tweet_length = max_tweet_length

    def vectorize(self, tweet):
        """
        Args:
            surname (str): the surname
        Returns:
            one_hot_matrix (np.ndarray): a matrix of one-hot vectors
        """

        one_hot_matrix_size = (len(self.tweet_vocab), self._max_tweet_length)
        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)
                               
        #for position_index, token in enumerate(surname):
         #   character_index = self.surname_vocab.lookup_token(character)
         #   one_hot_matrix[character_index][position_index] = 1

        token_indices = self.tweet_vocab.add_tweet(tweet)[0]
        position_indices = self.tweet_vocab.add_tweet(tweet)[1]

        for i in range(len(position_indices)):
          one_hot_matrix[token_index[i]][position_index[i]] = 1


        
        return one_hot_matrix

    @classmethod
    def from_dataframe(cls, tweets_df):
        """Instantiate the vectorizer from the dataset dataframe
        
        Args:
            surname_df (pandas.DataFrame): the surnames dataset
        Returns:
            an instance of the SurnameVectorizer
        """
        tokens_vocab = Vocabulary(unk_token="UNK")
        labels_vocab = Vocabulary(add_unk=True)
        max_tweet_length = 256

        for index, row in tweets_df.iterrows():
          tokens_vocab.add_tweet(row.Tweet)
          labels_vocab.add_token(row.Label)

        return cls(tokens_vocab, labels_vocab, max_surname_length)

class TokensVocabulary:
    def __init__(self, add_unk=True):
        self._token_to_ids = {}
        self._ids_to_token = {}

        # Add the unknown token and index
        if add_unk:
            self.unk_index = self.add_token("<UNK>")
        else:
            self.unk_index = -1

    def vocabulary_set(self):
        """this function returns a list of unique tokens"""
        return self._ids_to_token.values()

    def add_tweet(self, tweet):
        """Update mapping dicts based on the token.

        Args:
            tweet (str): the tweet to add into the Vocabulary
        Returns:
            index (int): the integer corresponding to the token
        """

        token_indices = []
        position_indices = []

        if ' ' in tweet:
          tokens = tweet.split()
        else:
          #remove the japanese space ideographic space)
          tokens = [x for x in tweet.replace('\u3000', '')]
        
        for i in range(len(tokens)): 
          if tokens[i] in self._token_to_ids:
            index = self._token_to_ids[tokens[i]]
          else:
            index = len(self._token_to_ids)
            self._token_to_ids[tokens[i]] = index
            self._ids_to_token[index] = tokens[i]
          token_indices.append(index)
          position_indices.append(i)
          
        return token_indices, position_indices
        #return index #replaced with the list of indices up there
    
    def add_token(self, token):
      """Update mapping dicts based on the token.

        Args:
            token (str): the item to add into the Vocabulary
        Returns:
            index (int): the integer corresponding to the token
        """
      if token in self._token_to_ids:
        index = self._token_to_ids[token]
      else:
        index = len(self._token_to_ids)
        self._token_to_ids[token] = index
        self._ids_to_token[index] = token
      return index


    def lookup_token(self, token):
      """Retrieve the index associated with the token
          or the UNK index if token isn't present.

        Args:
            token (str): the token to look up
        Returns:
            index (int): the index corresponding to the token
        Notes:
            `unk_index` needs to be >=0 (having been added into the Vocabulary)
              for the UNK functionality
      """
      if self.unk_index >= 0:
        return self._token_to_ids.get(token, self.unk_index)
      else:
        return self._token_to_ids[token]

    def lookup_index(self, index):
      """Return the token associated with the index

      Args:
          index (int): the index to look up
      Returns:
          token (str): the token corresponding to the index
      Raises:
          KeyError: if the index is not in the Vocabulary
      """
      if index not in self._ids_to_token:
        raise KeyError("the index (%d) is not in the Vocabulary" % index)
      return self._ids_to_token[index]

    def __len__(self):
      return len(self._token_to_ids)

class LabelsVocabulary:
    def __init__(self, add_unk=True):
        self._labels_to_ids = {}
        self._ids_to_labels = {}

        # Add the unknown token and index
        if add_unk:
            self.unk_index = self.add_label("<UNK>")
        else:
            self.unk_index = -1

    def vocabulary_set(self):
        """this function returns a list of unique tokens"""
        return self._ids_to_labels.values()

    def add_label(self, label):
        """Update mapping dicts based on the token.

        Args:
            tweet (str): the tweet to add into the Vocabulary
        Returns:
            index (int): the integer corresponding to the token
        """

        if label in self._labels_to_ids:
          index = self._labels_to_ids[label]
        else:
          index = len(self._labels_to_ids)
          self._labels_to_ids[label] = index
          self._ids_to_labels[index] = label
  
        return index

    def lookup_token(self, label):
        """Retrieve the index associated with the token
          or the UNK index if token isn't present.

        Args:
            labels (str): the label to look up
        Returns:
            index (int): the index corresponding to the label
        Notes:
            `unk_index` needs to be >=0 (having been added into the Vocabulary)
              for the UNK functionality
        """
        if self.unk_index >= 0:
            return self._labels_to_ids.get(label, self.unk_index)
        else:
            return self._labels_to_ids[label]

    def lookup_index(self, index):
        """Return the token associated with the index

        Args:
            index (int): the index to look up
        Returns:
            token (str): the token corresponding to the index
        Raises:
            KeyError: if the index is not in the Vocabulary
        """
        if index not in self._ids_to_labels:
            raise KeyError("the index (%d) is not in the Vocabulary" % index)
        return self._ids_to_labels[index]

    def __len__(self):
        return len(self._labels_to_ids)

dataset = TweetsDataset.load_and_create_dataset()

dataset._target_df

labelsvocab = LabelsVocabulary()

tokensvocab = TokensVocabulary()

vectorizer = Vectorizer(tokensvocab, labelsvocab)

"""All this that comes after, so far, is purely copy pasted from the Github for this in Rao&McMahan : https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_4/4_4_cnn_surnames/4_4_Classifying_Surnames_with_a_CNN.ipynb.
It will be updated afterwards :-)
"""

class TweetsClassifier(nn.Module):
    def __init__(self, initial_num_channels, num_classes, num_channels):
      """
      Args:
          initial_num_channels (int): size of the incoming feature vector
          num_classes (int): size of the output prediction vector
          num_channels (int): constant channel size to use throughout network
      """
      super(TweetsClassifier, self).__init__()
        
      self.convnet = nn.Sequential(
          nn.Conv1d(in_channels=initial_num_channels, 
                    out_channels=num_channels, kernel_size=3),
          nn.ELU(),
          nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3, stride=2),
          nn.ELU(),
          nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3, stride=2),
          nn.ELU(),
          nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3),
          nn.ELU()
        )
      self.fc = nn.Linear(num_channels, num_classes)

    def forward(self, x_surname, apply_softmax=False):
      """The forward pass of the classifier
        
      Args:
          x_surname (torch.Tensor): an input data tensor. 
              x_surname.shape should be (batch, initial_num_channels, max_surname_length)
          apply_softmax (bool): a flag for the softmax activation
              should be false if used with the Cross Entropy losses
      Returns:
          the resulting tensor. tensor.shape should be (batch, num_classes)
      """
      features = self.convnet(x_surname).squeeze(dim=2)
       
      prediction_vector = self.fc(features)

      if apply_softmax:
        prediction_vector = F.softmax(prediction_vector, dim=1)

      return prediction_vector

def make_train_state(args):
    return {'stop_early': False,
            'early_stopping_step': 0,
            'early_stopping_best_val': 1e8,
            'learning_rate': args.learning_rate,
            'epoch_index': 0,
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'test_loss': -1,
            'test_acc': -1,
            'model_filename': args.model_state_file}

def update_train_state(args, model, train_state):
    """Handle the training state updates.

    Components:
     - Early Stopping: Prevent overfitting.
     - Model Checkpoint: Model is saved if the model is better

    :param args: main arguments
    :param model: model to train
    :param train_state: a dictionary representing the training state values
    :returns:
        a new train_state
    """

    # Save one model at least
    if train_state['epoch_index'] == 0:
        torch.save(model.state_dict(), train_state['model_filename'])
        train_state['stop_early'] = False

    # Save model if performance improved
    elif train_state['epoch_index'] >= 1:
        loss_tm1, loss_t = train_state['val_loss'][-2:]

        # If loss worsened
        if loss_t >= train_state['early_stopping_best_val']:
            # Update step
            train_state['early_stopping_step'] += 1
        # Loss decreased
        else:
            # Save the best model
            if loss_t < train_state['early_stopping_best_val']:
                torch.save(model.state_dict(), train_state['model_filename'])

            # Reset early stopping step
            train_state['early_stopping_step'] = 0

        # Stop early ?
        train_state['stop_early'] = \
            train_state['early_stopping_step'] >= args.early_stopping_criteria

    return train_state

def compute_accuracy(y_pred, y_target):
    y_pred_indices = y_pred.max(dim=1)[1]
    n_correct = torch.eq(y_pred_indices, y_target).sum().item()
    return n_correct / len(y_pred_indices) * 100

args = Namespace(
# Data and Path information
    tweets_json="tweets.json",
    model_state_file="model.pth",
    save_dir=".",
    # Model hyper parameters
    hidden_dim=2,
    num_channels=256,
    # Training hyper parameters
    seed=1337,
    learning_rate=0.001,
    batch_size=128,
    num_epochs=1,
    early_stopping_criteria=5,
    dropout_p=0.3,
    # Runtime options
    cuda=False,
    reload_from_files=False,
    expand_filepaths_to_save_dir=True,
    catch_keyboard_interrupt=True
)


if args.expand_filepaths_to_save_dir:

  args.model_state_file = os.path.join(args.save_dir,
                                         args.model_state_file)
    
  print("Expanded filepaths: ")
  print("\t{}".format(args.model_state_file))
    
# Check CUDA
if not torch.cuda.is_available():
  args.cuda = False
  
args.device = torch.device("cuda" if args.cuda else "cpu")
print("Using CUDA: {}".format(args.cuda))

def set_seed_everywhere(seed, cuda):
  np.random.seed(seed)
  torch.manual_seed(seed)
  if cuda:
    torch.cuda.manual_seed_all(seed)

def handle_dirs(dirpath):
  if not os.path.exists(dirpath):
    os.makedirs(dirpath)
        
# Set seed for reproducibility
set_seed_everywhere(args.seed, args.cuda)

# handle dirs
handle_dirs(args.save_dir)

# create dataset and vectorizer
dataset = TweetsDataset.load_and_create_dataset()
    
vectorizer = dataset.get_vectorizer()

classifier = TweetsClassifier(initial_num_channels=len(vectorizer.token_vocab), 
                               num_classes=len(vectorizer.label_vocab),
                               num_channels=args.num_channels)

classifer = classifier.to(args.device)
#dataset.class_weights = dataset.class_weights.to(args.device)

loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)
optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,
                                           mode='min', factor=0.5,
                                           patience=1)

train_state = make_train_state(args)