{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# File paths\n",
    "TWEETS_FP = DATA_FP + \"tweets.json\"\n",
    "TRAIN_DEV_FP = DATA_FP + \"labels-train+dev.tsv\"\n",
    "TEST_FP = DATA_FP + \"labels-test.tsv\"\n",
    "\n",
    "# Column names\n",
    "COL_ID = 'ID'\n",
    "COL_TWEET = 'Tweet'\n",
    "COL_LABEL = 'Label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data\n",
    "\n",
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the first file (Tweets)\n",
    "tweets = []\n",
    "with open(TWEETS_FP, 'r') as tweets_fh:  # Tweets file handle\n",
    "    for line in tweets_fh:   # put each line in a list of lines\n",
    "        j_content = json.loads(line)\n",
    "        tweets.append(j_content)\n",
    "\n",
    "tweets = pd.DataFrame(tweets, columns=[COL_ID, COL_TWEET])  # make a dataframe out of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with both label documents\n",
    "\n",
    "train_dev_labels = pd.read_csv(TRAIN_DEV_FP, sep='\\t', header=None, names=[COL_LABEL, COL_ID])\n",
    "test_labels = pd.read_csv(TEST_FP, sep='\\t', header=None, names=[COL_LABEL, COL_ID])\n",
    "\n",
    "# deal with class imbalance in the train set\n",
    "lang_occurence = train_dev_labels.groupby(COL_LABEL).size()\n",
    "MIN_NR_OCCURENCES = 5  # minimum number of instances that we require to be present in the training set for a given language to be included in fitting of the model\n",
    "balanced_languages = lang_occurence.where(lambda x: x >= MIN_NR_OCCURENCES).dropna().index.values\n",
    "balanced_labels = train_dev_labels.Label.isin(balanced_languages)\n",
    "\n",
    "# Option 1 - replace rows that are labelled with an imbalanced language\n",
    "train_dev_labels.loc[~balanced_labels, 'Label'] = 'unknown'  # ~ is the element-wise logical not\n",
    "\n",
    "# Option 2 - keep the rows that are labelled with a balanced language\n",
    "# train_dev_labels = train_dev_labels[balanced_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[COL_ID] = tweets[COL_ID].astype(int) # to allow for merge, need the same type\n",
    "\n",
    "train_dev_data = pd.merge(tweets, train_dev_labels, on=COL_ID) # merge by ID\n",
    "test_data = pd.merge(tweets, test_labels, on=COL_ID) # merge by ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_n_shuffle(data):\n",
    "    data_no_na = data.dropna().copy()\n",
    "    return data_no_na.sample(frac=1)\n",
    "\n",
    "train_dev_data_prepared = drop_n_shuffle(train_dev_data).reset_index(drop = True)\n",
    "train_set = train_dev_data_prepared.sample(frac=0.9, random_state=0) # take 90% of the data, reshuffle\n",
    "test_set = drop_n_shuffle(test_data)\n",
    "dev_set = train_dev_data_prepared.drop(train_set.index) # take 10% that remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the ID columns, not needed anymore\n",
    "\n",
    "train = train_set.drop(COL_ID, axis=1)\n",
    "dev = dev_set.drop(COL_ID, axis=1)\n",
    "test = test_set.drop(COL_ID, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.Tweet \n",
    "y_train = train.Label\n",
    "X_dev = dev.Tweet\n",
    "y_dev = dev.Label\n",
    "X_test = test.Tweet\n",
    "y_test = test.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Language classification with linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average word length extractor, inspired  by https://michelleful.github.io/code-blog/2015/06/20/pipelines/)\n",
    "class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts tweet column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_word_length(self, tweet):\n",
    "        \"\"\"Helper code to compute average word length of a tweet\"\"\"\n",
    "        return np.mean([len(word) for word in tweet.split()])\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        # the result of the transform needs to be a 2d array a.k.a. dataframe\n",
    "        # https://stackoverflow.com/a/50713209\n",
    "        result = df.apply(self.average_word_length).to_frame()\n",
    "        return result\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_NB = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('nb_clf', MultinomialNB(fit_prior=False)) # classifier\n",
    "])\n",
    "\n",
    "param_grid1 = {'nb_clf__alpha': [0.2, 0.4, 0.6],\n",
    "                'features__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3), (1, 4)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:  2.2min remaining:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('ngram_tfidf', Pipeline(memory=None,\n",
       "     steps=[('ngram', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max...transformer_weights=None)), ('nb_clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'nb_clf__alpha': [0.2, 0.4, 0.6], 'features__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3), (1, 4)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_NB = GridSearchCV(pipeline_NB, param_grid1, cv=4, n_jobs=-1, verbose=10)\n",
    "gs_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500093510379653"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_NB = gs_NB.predict(X_dev)\n",
    "accuracy_score(y_dev, y_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_features__ngram_tfidf__ngram__ngram_range</th>\n",
       "      <th>param_nb_clf__alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.843938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.842047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.840073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.835855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.833195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.831719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.830286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.827584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.825735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score param_features__ngram_tfidf__ngram__ngram_range  \\\n",
       "0                1                                          (1, 2)   \n",
       "3                2                                          (1, 3)   \n",
       "6                3                                          (1, 4)   \n",
       "1                4                                          (1, 2)   \n",
       "4                5                                          (1, 3)   \n",
       "7                6                                          (1, 4)   \n",
       "2                7                                          (1, 2)   \n",
       "5                8                                          (1, 3)   \n",
       "8                9                                          (1, 4)   \n",
       "\n",
       "  param_nb_clf__alpha  mean_test_score  \n",
       "0                 0.2         0.843938  \n",
       "3                 0.2         0.842047  \n",
       "6                 0.2         0.840073  \n",
       "1                 0.4         0.835855  \n",
       "4                 0.4         0.833195  \n",
       "7                 0.4         0.831719  \n",
       "2                 0.6         0.830286  \n",
       "5                 0.6         0.827584  \n",
       "8                 0.6         0.825735  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame.from_dict(gs_NB.cv_results_)\n",
    "res.sort_values(by='rank_test_score')[['rank_test_score', 'param_features__ngram_tfidf__ngram__ngram_range', 'param_nb_clf__alpha', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500093510379653"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test three best models on the dev_set\n",
    "best_model1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('nb_clf', MultinomialNB(fit_prior=False, alpha=0.2)) # classifier\n",
    "])\n",
    "\n",
    "best_model1.fit(X_train, y_train)\n",
    "    \n",
    "y_NB1 = best_model1.predict(X_dev)\n",
    "accuracy_score(y_dev, y_NB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479521226856181"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word', ngram_range=(1, 3))),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('nb_clf', MultinomialNB(fit_prior=False, alpha=0.2)) # classifier\n",
    "])\n",
    "\n",
    "best_model2.fit(X_train, y_train)\n",
    "    \n",
    "y_NB2 = best_model3.predict(X_dev)\n",
    "accuracy_score(y_dev, y_NB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479521226856181"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model3 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word', ngram_range=(1, 4))),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('nb_clf', MultinomialNB(fit_prior=False, alpha=0.2)) # classifier\n",
    "])\n",
    "\n",
    "best_model3.fit(X_train, y_train)\n",
    "    \n",
    "y_NB3 = best_model3.predict(X_dev)\n",
    "accuracy_score(y_dev, y_NB3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8511002081474873"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chose the first model, since it perforemd best on the train set and dev set\n",
    "#end test\n",
    "y_NB_test = best_model1.predict(X_test)\n",
    "accuracy_score(y_test, y_NB_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_SGD = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge' ))# classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param_SGD = {'SGD_clf__penalty': ['none', 'l2'],\n",
    "                  'SGD_clf__max_iter': [100, 150, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed: 16.3min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 20.7min finished\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feats', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('ngram_tfidf', Pipeline(memory=None,\n",
       "     steps=[('ngram', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'SGD_clf__penalty': ['none', 'l2'], 'SGD_clf__max_iter': [100, 150, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_SGD = GridSearchCV(pipeline_SGD, grid_param_SGD, cv=4, n_jobs=-1, verbose=10)\n",
    "gs_SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479521226856181"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_SGD = gs_SGD.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_SGD_clf__max_iter</th>\n",
       "      <th>param_SGD_clf__penalty</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>none</td>\n",
       "      <td>0.843440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>0.842442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>none</td>\n",
       "      <td>0.841819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.798450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.798450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>150</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.798429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score param_SGD_clf__max_iter param_SGD_clf__penalty  \\\n",
       "4                1                     200                   none   \n",
       "2                2                     150                   none   \n",
       "0                3                     100                   none   \n",
       "1                4                     100                     l2   \n",
       "5                4                     200                     l2   \n",
       "3                6                     150                     l2   \n",
       "\n",
       "   mean_test_score  \n",
       "4         0.843440  \n",
       "2         0.842442  \n",
       "0         0.841819  \n",
       "1         0.798450  \n",
       "5         0.798450  \n",
       "3         0.798429  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame.from_dict(gs_SGD.cv_results_)\n",
    "res.sort_values(by='rank_test_score')[['rank_test_score', 'param_SGD_clf__max_iter', 'param_SGD_clf__penalty', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8490742472414438"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test three best models on the dev set\n",
    "best_SGD1 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=200, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD1.fit(X_train, y_train)\n",
    "\n",
    "y_SGD1 = best_SGD1.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.847204039648401"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD2 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=150, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD2.fit(X_train, y_train)\n",
    "\n",
    "y_SGD2 = best_SGD2.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8485131849635309"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD3 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=100, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD3.fit(X_train, y_train)\n",
    "\n",
    "y_SGD3 = best_SGD3.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  16 | elapsed: 11.1min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  16 | elapsed: 15.3min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 15.6min finished\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feats', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('ngram_tfidf', Pipeline(memory=None,\n",
       "     steps=[('ngram', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'feats__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3)], 'SGD_clf__max_iter': [200, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try one more Grid Search with playing around with ngrams!\n",
    "\n",
    "new_pipeline_SGD = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', penalty = None ))# classifier\n",
    "])\n",
    "\n",
    "new_grid = {'feats__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3)],\n",
    "                  'SGD_clf__max_iter': [200, 300]}\n",
    "\n",
    "new_gs_SGD = GridSearchCV(new_pipeline_SGD, new_grid, cv=4, n_jobs=-1, verbose=10)\n",
    "new_gs_SGD.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8556199738170936"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_SGD = new_gs_SGD.predict(X_dev)\n",
    "accuracy_score(new_y_SGD, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_SGD_clf__max_iter</th>\n",
       "      <th>param_feats__ngram_tfidf__ngram__ngram_range</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.848905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.848718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.844936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.844915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score param_SGD_clf__max_iter  \\\n",
       "2                1                     300   \n",
       "0                2                     200   \n",
       "3                3                     300   \n",
       "1                4                     200   \n",
       "\n",
       "  param_feats__ngram_tfidf__ngram__ngram_range  mean_test_score  \n",
       "2                                       (1, 2)         0.848905  \n",
       "0                                       (1, 2)         0.848718  \n",
       "3                                       (1, 3)         0.844936  \n",
       "1                                       (1, 3)         0.844915  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_res = pd.DataFrame.from_dict(new_gs_SGD.cv_results_)\n",
    "new_res.sort_values(by='rank_test_score')[['rank_test_score', 'param_SGD_clf__max_iter', 'param_feats__ngram_tfidf__ngram__ngram_range', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8537497662240509"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD4 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=300, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD4.fit(X_train, y_train)\n",
    "\n",
    "y_SGD4 = best_SGD4.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8541238077426594"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD5 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=200, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD5.fit(X_train, y_train)\n",
    "\n",
    "y_SGD5 = best_SGD5.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_SGD6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2a729e1d0ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0my_SGD5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_SGD6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_SGD6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_SGD6' is not defined"
     ]
    }
   ],
   "source": [
    "best_SGD6 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 3), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=300, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD6.fit(X_train, y_train)\n",
    "\n",
    "y_SGD5 = best_SGD6.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8498223302786609"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_SGD6 = best_SGD6.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8528146624275295"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD_tent1 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=400, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD_tent1.fit(X_train, y_train)\n",
    "\n",
    "y_SGD_tent1 = best_SGD_tent1.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD_tent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SGD_tent2 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 3), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=400, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD_tent2.fit(X_train, y_train)\n",
    "\n",
    "y_SGD_tent2 = best_SGD_tent2.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD_tent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_CLF = 'MLP_clf'\n",
    "\n",
    "pipeline_MLP = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    (MLP_CLF, MLPClassifier()) \n",
    "])\n",
    "\n",
    "grid_param_MLP = { MLP_CLF + '__hidden_layer_sizes': [(25,)],\n",
    "                   MLP_CLF + '__activation': ['relu'],\n",
    "                   MLP_CLF + '__solver': ['adam'],\n",
    "                   MLP_CLF + '__max_iter': [20],\n",
    "                   MLP_CLF + '__momentum': [0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed: 96.2min remaining: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 96.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 96.5min finished\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('ngram_tfidf',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('ngram',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         ma...\n",
       "                                                      solver='adam', tol=0.0001,\n",
       "                                                      validation_fraction=0.1,\n",
       "                                                      verbose=False,\n",
       "                                                      warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'MLP_clf__activation': ['relu'],\n",
       "                         'MLP_clf__hidden_layer_sizes': [(25,)],\n",
       "                         'MLP_clf__max_iter': [20], 'MLP_clf__momentum': [0.9],\n",
       "                         'MLP_clf__solver': ['adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_MLP = GridSearchCV(pipeline_MLP, grid_param_MLP, n_jobs=-1, verbose=10)\n",
    "gs_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mlp = gs_MLP.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814101365251543"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev, y_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_MLP_clf__activation</th>\n",
       "      <th>param_MLP_clf__hidden_layer_sizes</th>\n",
       "      <th>param_MLP_clf__max_iter</th>\n",
       "      <th>param_MLP_clf__momentum</th>\n",
       "      <th>param_MLP_clf__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277.731456</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>1.302234</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373.916279</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>1.333323</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.648922</td>\n",
       "      <td>0.673115</td>\n",
       "      <td>0.661007</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>357.605429</td>\n",
       "      <td>0.338305</td>\n",
       "      <td>1.623696</td>\n",
       "      <td>0.034237</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>474.193237</td>\n",
       "      <td>0.280791</td>\n",
       "      <td>1.402589</td>\n",
       "      <td>0.120243</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.597974</td>\n",
       "      <td>0.627356</td>\n",
       "      <td>0.612651</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264.683728</td>\n",
       "      <td>0.129365</td>\n",
       "      <td>1.416832</td>\n",
       "      <td>0.077422</td>\n",
       "      <td>relu</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>353.529966</td>\n",
       "      <td>0.124757</td>\n",
       "      <td>1.388335</td>\n",
       "      <td>0.100204</td>\n",
       "      <td>relu</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.668937</td>\n",
       "      <td>0.740422</td>\n",
       "      <td>0.704647</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>340.663349</td>\n",
       "      <td>0.152085</td>\n",
       "      <td>1.470408</td>\n",
       "      <td>0.085144</td>\n",
       "      <td>relu</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>547.581941</td>\n",
       "      <td>0.392927</td>\n",
       "      <td>1.741871</td>\n",
       "      <td>0.329597</td>\n",
       "      <td>relu</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.600507</td>\n",
       "      <td>0.705021</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.052257</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     277.731456      0.010334         1.302234        0.010232   \n",
       "1     373.916279      0.055184         1.333323        0.013040   \n",
       "2     357.605429      0.338305         1.623696        0.034237   \n",
       "3     474.193237      0.280791         1.402589        0.120243   \n",
       "4     264.683728      0.129365         1.416832        0.077422   \n",
       "5     353.529966      0.124757         1.388335        0.100204   \n",
       "6     340.663349      0.152085         1.470408        0.085144   \n",
       "7     547.581941      0.392927         1.741871        0.329597   \n",
       "\n",
       "  param_MLP_clf__activation param_MLP_clf__hidden_layer_sizes  \\\n",
       "0                      tanh                            (4, 3)   \n",
       "1                      tanh                            (4, 3)   \n",
       "2                      tanh                            (5, 3)   \n",
       "3                      tanh                            (5, 3)   \n",
       "4                      relu                            (4, 3)   \n",
       "5                      relu                            (4, 3)   \n",
       "6                      relu                            (5, 3)   \n",
       "7                      relu                            (5, 3)   \n",
       "\n",
       "  param_MLP_clf__max_iter param_MLP_clf__momentum param_MLP_clf__solver  \\\n",
       "0                      50                     0.9                   sgd   \n",
       "1                      50                     0.9                  adam   \n",
       "2                      50                     0.9                   sgd   \n",
       "3                      50                     0.9                  adam   \n",
       "4                      50                     0.9                   sgd   \n",
       "5                      50                     0.9                  adam   \n",
       "6                      50                     0.9                   sgd   \n",
       "7                      50                     0.9                  adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.350828   \n",
       "1  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.648922   \n",
       "2  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.350828   \n",
       "3  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.597974   \n",
       "4  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.350828   \n",
       "5  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.668937   \n",
       "6  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.350828   \n",
       "7  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.600507   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.351429         0.351128        0.000300                5  \n",
       "1           0.673115         0.661007        0.012096                2  \n",
       "2           0.351429         0.351128        0.000300                5  \n",
       "3           0.627356         0.612651        0.014691                4  \n",
       "4           0.351429         0.351128        0.000300                5  \n",
       "5           0.740422         0.704647        0.035743                1  \n",
       "6           0.351429         0.351128        0.000300                5  \n",
       "7           0.705021         0.652716        0.052257                3  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame.from_dict(gs_MLP.cv_results_)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
