{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# File paths\n",
    "TWEETS_FP = \"tweets.json\"\n",
    "TRAIN_DEV_FP = \"labels-train+dev.tsv\"\n",
    "TEST_FP = \"labels-test.tsv\"\n",
    "\n",
    "# Column names\n",
    "COL_ID = 'ID'\n",
    "COL_TWEET = 'Tweet'\n",
    "COL_LABEL = 'Label'\n",
    "\n",
    "# minimum number of instances that we require to be present in the training set\n",
    "# for a given language to be included in fitting of the model\n",
    "MIN_NR_OCCURENCES = 10\n",
    "\n",
    "# unknown class name\n",
    "CLASS_UNK = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data\n",
    "\n",
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    \"\"\"Return a dataframe of tweets\"\"\"\n",
    "    tweets = []\n",
    "    with open(TWEETS_FP, 'r') as tweets_fh:  # Tweets file handle\n",
    "        for line in tweets_fh:   # put each line in a list of lines\n",
    "            j_content = json.loads(line)\n",
    "            tweets.append(j_content)\n",
    "\n",
    "    # make a dataframe out of it\n",
    "    tweets = pd.DataFrame(tweets, columns=[COL_ID, COL_TWEET])\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_labels():\n",
    "    \"\"\"Return a dataframe of train_dev labels\"\"\"\n",
    "    train_dev_labels = pd.read_csv(\n",
    "        TRAIN_DEV_FP,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=[COL_LABEL, COL_ID]\n",
    "    )\n",
    "    # remove whitespace from labels (e.g. \"ar  \" should be equal to \"ar\")\n",
    "    train_dev_labels.Label = train_dev_labels.Label.str.strip()\n",
    "\n",
    "    # deal with class imbalance in the train set\n",
    "    lang_occurence = train_dev_labels.groupby(COL_LABEL).size()\n",
    "    balanced_languages = lang_occurence.where(\n",
    "        lang_occurence >= MIN_NR_OCCURENCES\n",
    "    ).dropna().index.values\n",
    "    balanced_labels = train_dev_labels.Label.isin(balanced_languages)\n",
    "\n",
    "\n",
    "    # Option 1 - replace rows that are labelled with an imbalanced language\n",
    "    # ~ is element-wise logical not\n",
    "    train_dev_labels.loc[~balanced_labels, COL_LABEL] = CLASS_UNK\n",
    "\n",
    "    # Option 2 - keep the rows that are labelled with a balanced language\n",
    "    # train_dev_labels = train_dev_labels[balanced_labels]\n",
    "    return train_dev_labels\n",
    "\n",
    "def get_test_labels():\n",
    "    \"\"\"Return a dataframe of test labels\"\"\"\n",
    "    return pd.read_csv(\n",
    "        TEST_FP,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=[COL_LABEL, COL_ID]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets(tweets, train_dev_labels, test_labels, use_dev=True):\n",
    "    \"\"\"Return a tuple of dataframes comprising three main data sets\"\"\"\n",
    "    # to allow for merge, need the same type\n",
    "    tweets[COL_ID] = tweets[COL_ID].astype(int)\n",
    "\n",
    "    # Merge by ID\n",
    "    train_dev_data = pd.merge(tweets, train_dev_labels, on=COL_ID)\n",
    "    test_data = pd.merge(tweets, test_labels, on=COL_ID)\n",
    "\n",
    "    # Util function\n",
    "    def drop_n_shuffle(data):\n",
    "        data_no_na = data.dropna().copy()\n",
    "        return data_no_na.sample(frac=1)\n",
    "\n",
    "    frac = 1\n",
    "    if use_dev:\n",
    "        frac = 0.9\n",
    "\n",
    "    train_dev_data_prepared = drop_n_shuffle(\n",
    "        train_dev_data\n",
    "    ).reset_index(drop=True)\n",
    "    # take 90% of the data, reshuffle\n",
    "    train_set = train_dev_data_prepared.sample(frac=frac, random_state=0)\n",
    "    # take 10% that remain\n",
    "    dev_set = train_dev_data_prepared.drop(train_set.index)\n",
    "    test_set = drop_n_shuffle(test_data)\n",
    "\n",
    "    # drop the ID columns, not needed anymore\n",
    "    train = train_set.drop(COL_ID, axis=1)\n",
    "    dev = dev_set.drop(COL_ID, axis=1)\n",
    "    test = test_set.drop(COL_ID, axis=1)\n",
    "\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\"Return the three main sets used in the pipelines\"\"\"\n",
    "    tweets = get_tweets()\n",
    "    train_labels = get_train_labels()\n",
    "    test_labels = get_test_labels()\n",
    "    return create_sets(tweets, train_labels, test_labels, use_dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = preprocess()\n",
    "\n",
    "X_train = train.Tweet \n",
    "y_train = train.Label\n",
    "X_dev = dev.Tweet\n",
    "y_dev = dev.Label\n",
    "X_test = test.Tweet\n",
    "y_test = test.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Language classification with linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average word length extractor, inspired  by https://michelleful.github.io/code-blog/2015/06/20/pipelines/)\n",
    "class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts tweet column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_word_length(self, tweet):\n",
    "        \"\"\"Helper code to compute average word length of a tweet\"\"\"\n",
    "        return np.mean([len(word) for word in tweet.split()])\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        # the result of the transform needs to be a 2d array a.k.a. dataframe\n",
    "        # https://stackoverflow.com/a/50713209\n",
    "        result = df.apply(self.average_word_length).to_frame()\n",
    "        return result\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_NB = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('nb_clf', MultinomialNB(fit_prior=False)) # classifier\n",
    "])\n",
    "\n",
    "param_grid1 = {'nb_clf__alpha': [0.2, 0.4, 0.6],\n",
    "                'features__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3), (1, 4)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:  2.2min remaining:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('ngram_tfidf', Pipeline(memory=None,\n",
       "     steps=[('ngram', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max...transformer_weights=None)), ('nb_clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'nb_clf__alpha': [0.2, 0.4, 0.6], 'features__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3), (1, 4)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gs_NB = GridSearchCV(pipeline_NB, param_grid1, cv=4, n_jobs=-1, verbose=10)\n",
    "# gs_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500093510379653"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_NB = gs_NB.predict(X_dev)\n",
    "# accuracy_score(y_dev, y_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_features__ngram_tfidf__ngram__ngram_range</th>\n",
       "      <th>param_nb_clf__alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.843938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.842047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.840073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.835855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.833195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.831719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.830286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.827584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.825735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score param_features__ngram_tfidf__ngram__ngram_range  \\\n",
       "0                1                                          (1, 2)   \n",
       "3                2                                          (1, 3)   \n",
       "6                3                                          (1, 4)   \n",
       "1                4                                          (1, 2)   \n",
       "4                5                                          (1, 3)   \n",
       "7                6                                          (1, 4)   \n",
       "2                7                                          (1, 2)   \n",
       "5                8                                          (1, 3)   \n",
       "8                9                                          (1, 4)   \n",
       "\n",
       "  param_nb_clf__alpha  mean_test_score  \n",
       "0                 0.2         0.843938  \n",
       "3                 0.2         0.842047  \n",
       "6                 0.2         0.840073  \n",
       "1                 0.4         0.835855  \n",
       "4                 0.4         0.833195  \n",
       "7                 0.4         0.831719  \n",
       "2                 0.6         0.830286  \n",
       "5                 0.6         0.827584  \n",
       "8                 0.6         0.825735  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame.from_dict(gs_NB.cv_results_)\n",
    "res.sort_values(by='rank_test_score')[['rank_test_score', 'param_features__ngram_tfidf__ngram__ngram_range', 'param_nb_clf__alpha', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_MNB(X_train, y_train):\n",
    "    \"\"\"Return the Multinomial Na√Øve Bayes model trained on the parameters\"\"\"\n",
    "    multinomial_NB = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('ngram_tfidf', Pipeline([\n",
    "                ('ngram', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('ave_scaled', Pipeline([\n",
    "                ('ave', AverageWordLengthExtractor()),\n",
    "                ('scale', MinMaxScaler())\n",
    "            ]))\n",
    "        ])),\n",
    "        ('nb_clf', MultinomialNB(fit_prior=False, alpha=0.1)) # classifier\n",
    "    ])\n",
    "    # train\n",
    "    multinomial_NB.fit(X_train, y_train)\n",
    "    return multinomial_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full, _, _ = preprocess() # no dev set split\n",
    "\n",
    "X_train_full = train_full.Tweet\n",
    "y_train_full = train_full.Label\n",
    "\n",
    "best_nb = train_and_predict_MNB(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776390127862028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[513,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_nb_predicted = best_nb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, best_nb_predicted)\n",
    "print(accuracy)\n",
    "best_nb_conf_mx = confusion_matrix(y_test, best_nb_predicted)\n",
    "best_nb_conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4728 1452 2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>4823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja</th>\n",
       "      <td>2505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>1495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>und</th>\n",
       "      <td>1244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>th</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ms</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ur</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vi</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lv</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh-TW</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi-Latn</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ta</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar_LATN</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>az</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ro</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sw</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xh</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ur_LATN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ta_LATN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yo</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh-CN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bs</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>km</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jv</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ht</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja_LATN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mn</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mk</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko_LATN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zu</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Label        \n",
       "en       4823\n",
       "ja       2505\n",
       "es       1495\n",
       "und      1244\n",
       "id        827\n",
       "pt        711\n",
       "ar        542\n",
       "ru        244\n",
       "fr        227\n",
       "tr        176\n",
       "ko        110\n",
       "th         99\n",
       "tl         90\n",
       "it         77\n",
       "de         50\n",
       "nl         43\n",
       "ms         32\n",
       "pl         26\n",
       "sv         15\n",
       "he         14\n",
       "el         12\n",
       "fi          8\n",
       "sr          7\n",
       "ur          5\n",
       "vi          5\n",
       "lv          5\n",
       "fa          5\n",
       "zh-TW       4\n",
       "hi          4\n",
       "hi-Latn     4\n",
       "...       ...\n",
       "ta          3\n",
       "ar_LATN     3\n",
       "az          2\n",
       "bg          2\n",
       "uk          2\n",
       "ro          2\n",
       "eu          2\n",
       "sw          2\n",
       "xh          1\n",
       "ur_LATN     1\n",
       "ta_LATN     1\n",
       "yo          1\n",
       "zh-CN       1\n",
       "cs          1\n",
       "da          1\n",
       "bs          1\n",
       "km          1\n",
       "jv          1\n",
       "sk          1\n",
       "hr          1\n",
       "no          1\n",
       "ht          1\n",
       "ne          1\n",
       "ja_LATN     1\n",
       "mr          1\n",
       "mn          1\n",
       "mk          1\n",
       "la          1\n",
       "ko_LATN     1\n",
       "zu          1\n",
       "\n",
       "[61 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_lan = best_nb_conf_mx[10, 10]\n",
    "second_lan = best_nb_conf_mx[11, 11]\n",
    "third_lan = best_nb_conf_mx[23, 23]\n",
    "\n",
    "print(first_lan, second_lan, third_lan)\n",
    "\n",
    "test.groupby(COL_LABEL).size().to_frame().sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(best_nb_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_SGD = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge' ))# classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param_SGD = {'SGD_clf__penalty': ['none', 'l2'],\n",
    "                  'SGD_clf__max_iter': [100, 150, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed: 16.3min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 20.7min finished\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feats', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('ngram_tfidf', Pipeline(memory=None,\n",
       "     steps=[('ngram', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'SGD_clf__penalty': ['none', 'l2'], 'SGD_clf__max_iter': [100, 150, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_SGD = GridSearchCV(pipeline_SGD, grid_param_SGD, cv=4, n_jobs=-1, verbose=10)\n",
    "gs_SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479521226856181"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_SGD = gs_SGD.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_SGD_clf__max_iter</th>\n",
       "      <th>param_SGD_clf__penalty</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>none</td>\n",
       "      <td>0.843440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>0.842442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>none</td>\n",
       "      <td>0.841819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.798450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.798450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>150</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.798429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score param_SGD_clf__max_iter param_SGD_clf__penalty  \\\n",
       "4                1                     200                   none   \n",
       "2                2                     150                   none   \n",
       "0                3                     100                   none   \n",
       "1                4                     100                     l2   \n",
       "5                4                     200                     l2   \n",
       "3                6                     150                     l2   \n",
       "\n",
       "   mean_test_score  \n",
       "4         0.843440  \n",
       "2         0.842442  \n",
       "0         0.841819  \n",
       "1         0.798450  \n",
       "5         0.798450  \n",
       "3         0.798429  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame.from_dict(gs_SGD.cv_results_)\n",
    "res.sort_values(by='rank_test_score')[['rank_test_score', 'param_SGD_clf__max_iter', 'param_SGD_clf__penalty', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8490742472414438"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test three best models on the dev set\n",
    "best_SGD1 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=200, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD1.fit(X_train, y_train)\n",
    "\n",
    "y_SGD1 = best_SGD1.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.847204039648401"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD2 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=150, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD2.fit(X_train, y_train)\n",
    "\n",
    "y_SGD2 = best_SGD2.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8485131849635309"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD3 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=100, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD3.fit(X_train, y_train)\n",
    "\n",
    "y_SGD3 = best_SGD3.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  16 | elapsed: 11.1min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  16 | elapsed: 15.3min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 15.6min finished\n",
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feats', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('ngram_tfidf', Pipeline(memory=None,\n",
       "     steps=[('ngram', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'feats__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3)], 'SGD_clf__max_iter': [200, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try one more Grid Search with playing around with ngrams!\n",
    "\n",
    "new_pipeline_SGD = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', penalty = None ))# classifier\n",
    "])\n",
    "\n",
    "new_grid = {'feats__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 3)],\n",
    "                  'SGD_clf__max_iter': [200, 300]}\n",
    "\n",
    "new_gs_SGD = GridSearchCV(new_pipeline_SGD, new_grid, cv=4, n_jobs=-1, verbose=10)\n",
    "new_gs_SGD.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8556199738170936"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_SGD = new_gs_SGD.predict(X_dev)\n",
    "accuracy_score(new_y_SGD, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_SGD_clf__max_iter</th>\n",
       "      <th>param_feats__ngram_tfidf__ngram__ngram_range</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.848905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.848718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.844936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.844915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score param_SGD_clf__max_iter  \\\n",
       "2                1                     300   \n",
       "0                2                     200   \n",
       "3                3                     300   \n",
       "1                4                     200   \n",
       "\n",
       "  param_feats__ngram_tfidf__ngram__ngram_range  mean_test_score  \n",
       "2                                       (1, 2)         0.848905  \n",
       "0                                       (1, 2)         0.848718  \n",
       "3                                       (1, 3)         0.844936  \n",
       "1                                       (1, 3)         0.844915  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_res = pd.DataFrame.from_dict(new_gs_SGD.cv_results_)\n",
    "new_res.sort_values(by='rank_test_score')[['rank_test_score', 'param_SGD_clf__max_iter', 'param_feats__ngram_tfidf__ngram__ngram_range', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8056854310828502"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD4 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=300, penalty='l2' ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD4.fit(X_train, y_train)\n",
    "\n",
    "y_SGD4 = best_SGD4.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8515055171123995"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD4b = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=300, penalty='none', tol=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD4b.fit(X_train, y_train)\n",
    "\n",
    "y_SGD4b = best_SGD4b.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587570621468926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_SGD = best_SGD4.predict(X_test)\n",
    "accuracy_score(y_test, final_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8568986024382992"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final2 = best_SGD4b.predict(X_test)\n",
    "accuracy_score(y_test, final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8541238077426594"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD5 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=200, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD5.fit(X_train, y_train)\n",
    "\n",
    "y_SGD5 = best_SGD5.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_SGD6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2a729e1d0ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0my_SGD5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_SGD6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_SGD6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_SGD6' is not defined"
     ]
    }
   ],
   "source": [
    "best_SGD6 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 3), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=300, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD6.fit(X_train, y_train)\n",
    "\n",
    "y_SGD5 = best_SGD6.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8498223302786609"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_SGD6 = best_SGD6.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debora/Envs/nlppython/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8528146624275295"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_SGD_tent1 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 2), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=400, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD_tent1.fit(X_train, y_train)\n",
    "\n",
    "y_SGD_tent1 = best_SGD_tent1.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD_tent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SGD_tent2 = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 3), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier(loss='hinge', max_iter=400, penalty=None ))# classifier\n",
    "])\n",
    "\n",
    "best_SGD_tent2.fit(X_train, y_train)\n",
    "\n",
    "y_SGD_tent2 = best_SGD_tent2.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD_tent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_CLF = 'MLP_clf'\n",
    "\n",
    "pipeline_MLP = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    (MLP_CLF, MLPClassifier()) \n",
    "])\n",
    "\n",
    "grid_param_MLP = { MLP_CLF + '__hidden_layer_sizes': [(25,)],\n",
    "                   MLP_CLF + '__activation': ['relu'],\n",
    "                   MLP_CLF + '__solver': ['adam'],\n",
    "                   MLP_CLF + '__max_iter': [20],\n",
    "                   MLP_CLF + '__momentum': [0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed: 96.2min remaining: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 96.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 96.5min finished\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('ngram_tfidf',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('ngram',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         ma...\n",
       "                                                      solver='adam', tol=0.0001,\n",
       "                                                      validation_fraction=0.1,\n",
       "                                                      verbose=False,\n",
       "                                                      warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'MLP_clf__activation': ['relu'],\n",
       "                         'MLP_clf__hidden_layer_sizes': [(25,)],\n",
       "                         'MLP_clf__max_iter': [20], 'MLP_clf__momentum': [0.9],\n",
       "                         'MLP_clf__solver': ['adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_MLP = GridSearchCV(pipeline_MLP, grid_param_MLP, n_jobs=-1, verbose=10)\n",
    "gs_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mlp = gs_MLP.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814101365251543"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev, y_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_MLP_clf__activation</th>\n",
       "      <th>param_MLP_clf__hidden_layer_sizes</th>\n",
       "      <th>param_MLP_clf__max_iter</th>\n",
       "      <th>param_MLP_clf__momentum</th>\n",
       "      <th>param_MLP_clf__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277.731456</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>1.302234</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373.916279</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>1.333323</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.648922</td>\n",
       "      <td>0.673115</td>\n",
       "      <td>0.661007</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>357.605429</td>\n",
       "      <td>0.338305</td>\n",
       "      <td>1.623696</td>\n",
       "      <td>0.034237</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>474.193237</td>\n",
       "      <td>0.280791</td>\n",
       "      <td>1.402589</td>\n",
       "      <td>0.120243</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.597974</td>\n",
       "      <td>0.627356</td>\n",
       "      <td>0.612651</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264.683728</td>\n",
       "      <td>0.129365</td>\n",
       "      <td>1.416832</td>\n",
       "      <td>0.077422</td>\n",
       "      <td>relu</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>353.529966</td>\n",
       "      <td>0.124757</td>\n",
       "      <td>1.388335</td>\n",
       "      <td>0.100204</td>\n",
       "      <td>relu</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.668937</td>\n",
       "      <td>0.740422</td>\n",
       "      <td>0.704647</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>340.663349</td>\n",
       "      <td>0.152085</td>\n",
       "      <td>1.470408</td>\n",
       "      <td>0.085144</td>\n",
       "      <td>relu</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>547.581941</td>\n",
       "      <td>0.392927</td>\n",
       "      <td>1.741871</td>\n",
       "      <td>0.329597</td>\n",
       "      <td>relu</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.600507</td>\n",
       "      <td>0.705021</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.052257</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     277.731456      0.010334         1.302234        0.010232   \n",
       "1     373.916279      0.055184         1.333323        0.013040   \n",
       "2     357.605429      0.338305         1.623696        0.034237   \n",
       "3     474.193237      0.280791         1.402589        0.120243   \n",
       "4     264.683728      0.129365         1.416832        0.077422   \n",
       "5     353.529966      0.124757         1.388335        0.100204   \n",
       "6     340.663349      0.152085         1.470408        0.085144   \n",
       "7     547.581941      0.392927         1.741871        0.329597   \n",
       "\n",
       "  param_MLP_clf__activation param_MLP_clf__hidden_layer_sizes  \\\n",
       "0                      tanh                            (4, 3)   \n",
       "1                      tanh                            (4, 3)   \n",
       "2                      tanh                            (5, 3)   \n",
       "3                      tanh                            (5, 3)   \n",
       "4                      relu                            (4, 3)   \n",
       "5                      relu                            (4, 3)   \n",
       "6                      relu                            (5, 3)   \n",
       "7                      relu                            (5, 3)   \n",
       "\n",
       "  param_MLP_clf__max_iter param_MLP_clf__momentum param_MLP_clf__solver  \\\n",
       "0                      50                     0.9                   sgd   \n",
       "1                      50                     0.9                  adam   \n",
       "2                      50                     0.9                   sgd   \n",
       "3                      50                     0.9                  adam   \n",
       "4                      50                     0.9                   sgd   \n",
       "5                      50                     0.9                  adam   \n",
       "6                      50                     0.9                   sgd   \n",
       "7                      50                     0.9                  adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.350828   \n",
       "1  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.648922   \n",
       "2  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.350828   \n",
       "3  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.597974   \n",
       "4  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.350828   \n",
       "5  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.668937   \n",
       "6  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.350828   \n",
       "7  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.600507   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.351429         0.351128        0.000300                5  \n",
       "1           0.673115         0.661007        0.012096                2  \n",
       "2           0.351429         0.351128        0.000300                5  \n",
       "3           0.627356         0.612651        0.014691                4  \n",
       "4           0.351429         0.351128        0.000300                5  \n",
       "5           0.740422         0.704647        0.035743                1  \n",
       "6           0.351429         0.351128        0.000300                5  \n",
       "7           0.705021         0.652716        0.052257                3  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame.from_dict(gs_MLP.cv_results_)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
