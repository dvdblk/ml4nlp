{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# File paths\n",
    "DATA_FP = \"./data/\" # Data file path\n",
    "TWEETS_FP = DATA_FP + \"tweets.json\"\n",
    "TRAIN_DEV_FP = DATA_FP + \"labels-train+dev.tsv\"\n",
    "TEST_FP = DATA_FP + \"labels-test.tsv\"\n",
    "\n",
    "# Column names\n",
    "COL_ID = 'ID'\n",
    "COL_TWEET = 'Tweet'\n",
    "COL_LABEL = 'Label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data\n",
    "\n",
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the first file (Tweets)\n",
    "tweets = []\n",
    "with open(TWEETS_FP, 'r') as tweets_fh:  # Tweets file handle\n",
    "    for line in tweets_fh:   # put each line in a list of lines\n",
    "        j_content = json.loads(line)\n",
    "        tweets.append(j_content)\n",
    "\n",
    "tweets = pd.DataFrame(tweets, columns=[COL_ID, COL_TWEET])  # make a dataframe out of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with both label documents\n",
    "\n",
    "train_dev_labels = pd.read_csv(TRAIN_DEV_FP, sep='\\t', header=None, names=[COL_LABEL, COL_ID])\n",
    "test_labels = pd.read_csv(TEST_FP, sep='\\t', header=None, names=[COL_LABEL, COL_ID])\n",
    "\n",
    "# deal with class imbalance in the train set\n",
    "lang_occurence = train.groupby(COL_LABEL).size()\n",
    "MIN_NR_OCCURENCES = 5  # minimum number of instances that we require to be present in the training set for a given language to be included in fitting of the model\n",
    "balanced_languages = lang_occurence.where(lambda x: x >= MIN_NR_OCCURENCES).dropna().index.values\n",
    "balanced_labels = train_dev_labels.Label.isin(balanced_languages)\n",
    "\n",
    "# Option 1 - replace rows that are labelled with an imbalanced language\n",
    "train_dev_labels.loc[~balanced_labels, 'Label'] = 'unknown'  # ~ is the element-wise logical not\n",
    "\n",
    "# Option 2 - keep the rows that are labelled with a balanced language\n",
    "# train_dev_labels = train_dev_labels[balanced_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[COL_ID] = tweets[COL_ID].astype(int) # to allow for merge, need the same type\n",
    "\n",
    "train_dev_data = pd.merge(tweets, train_dev_labels, on=COL_ID) # merge by ID\n",
    "test_data = pd.merge(tweets, test_labels, on=COL_ID) # merge by ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_n_shuffle(data):\n",
    "    data_no_na = data.dropna().copy()\n",
    "    return data_no_na.sample(frac=1)\n",
    "\n",
    "train_dev_data_prepared = drop_n_shuffle(train_dev_data).reset_index(drop = True)\n",
    "train_set = train_dev_data_prepared.sample(frac=0.9, random_state=0) # take 90% of the data, reshuffle\n",
    "test_set = drop_n_shuffle(test_data)\n",
    "dev_set = train_dev_data_prepared.drop(train_set.index) # take 10% that remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the ID columns, not needed anymore\n",
    "\n",
    "train = train_set.drop(COL_ID, axis=1)\n",
    "dev = dev_set.drop(COL_ID, axis=1)\n",
    "test = test_set.drop(COL_ID, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.Tweet \n",
    "y_train = train.Label\n",
    "X_dev = dev.Tweet\n",
    "y_dev = dev.Label\n",
    "X_test = test.Tweet\n",
    "y_test = test.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Language classification with linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average word length extractor, inspired  by https://michelleful.github.io/code-blog/2015/06/20/pipelines/)\n",
    "class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts tweet column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_word_length(self, tweet):\n",
    "        \"\"\"Helper code to compute average word length of a tweet\"\"\"\n",
    "        return np.mean([len(word) for word in tweet.split()])\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        # the result of the transform needs to be a 2d array a.k.a. dataframe\n",
    "        # https://stackoverflow.com/a/50713209\n",
    "        result = df.apply(self.average_word_length).to_frame()\n",
    "        return result\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_NB1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('nb_clf', MultinomialNB()) # classifier\n",
    "])\n",
    "\n",
    "param_grid1 = {'nb_clf__alpha': [0.2, 0.6, 0.8, 1.0],\n",
    "                'nb_clf__fit_prior': [True, False],\n",
    "                'features__ngram_tfidf__ngram__ngram_range': [(1, 2), (1, 4)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('ngram_tfidf',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('ngram',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         ma...\n",
       "                                                     transformer_weights=None,\n",
       "                                                     verbose=False)),\n",
       "                                       ('nb_clf',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'features__ngram_tfidf__ngram__ngram_range': [(1, 2),\n",
       "                                                                       (1, 4)],\n",
       "                         'nb_clf__alpha': [0.2, 0.6, 0.8, 1.0],\n",
       "                         'nb_clf__fit_prior': [True, False]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_NB1 = GridSearchCV(pipeline_NB1, param_grid1, cv=4, n_jobs=-1, verbose=10)\n",
    "gs_NB1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8539367869833552"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_NB1 = gs_NB1.predict(X_dev)\n",
    "accuracy_score(y_dev, y_NB1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_SGD = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer()), \n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('SGD_clf', SGDClassifier())# classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param_SGD = {'SGD_clf__loss': ['hinge', 'log'],\n",
    "                  'SGD_clf__penalty': ['none', 'l1', 'l2'],\n",
    "                  'SGD_clf__max_iter': [100, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('feats',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('ngram_tfidf',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('ngram',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         max_f...\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      power_t=0.5,\n",
       "                                                      random_state=None,\n",
       "                                                      shuffle=True, tol=0.001,\n",
       "                                                      validation_fraction=0.1,\n",
       "                                                      verbose=0,\n",
       "                                                      warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'SGD_clf__loss': ['hinge', 'log'],\n",
       "                         'SGD_clf__max_iter': [100, 200],\n",
       "                         'SGD_clf__penalty': ['none', 'l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_SGD = GridSearchCV(pipeline_SGD, grid_param_SGD, cv=4, n_jobs=-1, verbose=10)\n",
    "gs_SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447727697774453"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_SGD = gs_SGD.predict(X_dev)\n",
    "accuracy_score(y_dev, y_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_CLF = 'MLP_clf'\n",
    "\n",
    "pipeline_MLP = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # first feature\n",
    "        ('ngram_tfidf', Pipeline([\n",
    "            ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='word')),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        # second feature\n",
    "        ('ave_scaled', Pipeline([\n",
    "            ('ave', AverageWordLengthExtractor()),\n",
    "            ('scale', MinMaxScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    (MLP_CLF, MLPClassifier()) \n",
    "])\n",
    "\n",
    "grid_param_MLP = { MLP_CLF + '__hidden_layer_sizes': [(25,)],\n",
    "                   MLP_CLF + '__activation': ['relu'],\n",
    "                   MLP_CLF + '__solver': ['adam'],\n",
    "                   MLP_CLF + '__max_iter': [20],\n",
    "                   MLP_CLF + '__momentum': [0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "gs_MLP = GridSearchCV(pipeline_MLP, grid_param_MLP, n_jobs=-1, verbose=10)\n",
    "gs_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Tweet\n",
      "1783    3.600000\n",
      "2475    5.444444\n",
      "1613    3.285714\n",
      "1532    4.272727\n",
      "10388  17.500000\n",
      "5237    7.058824\n",
      "1552    3.105263\n",
      "10653   3.750000\n",
      "12986   4.500000\n",
      "1997    4.166667\n",
      "6680    3.961538\n",
      "11063   4.230769\n",
      "536     4.863636\n",
      "11314   5.200000\n",
      "9952   12.666667\n",
      "7058    3.944444\n",
      "10795   7.333333\n",
      "5872    3.500000\n",
      "6013    4.625000\n",
      "10879   4.000000\n",
      "8265   18.000000\n",
      "3352    6.666667\n",
      "11500   4.200000\n",
      "8770   17.666667\n",
      "9       6.266667\n",
      "8474    1.000000\n",
      "11279   4.000000\n",
      "2361    3.722222\n",
      "12131  11.444444\n",
      "83      6.500000\n",
      "...          ...\n",
      "7799    7.272727\n",
      "7474    4.500000\n",
      "9197    2.900000\n",
      "3379    5.250000\n",
      "12327   7.500000\n",
      "5056    7.875000\n",
      "9537   12.500000\n",
      "378     4.714286\n",
      "3351    4.200000\n",
      "4400    5.000000\n",
      "12045   7.312500\n",
      "1880    4.833333\n",
      "6129    4.333333\n",
      "8377   10.200000\n",
      "7201   10.000000\n",
      "4114    4.423077\n",
      "12614  10.500000\n",
      "6535    5.562500\n",
      "2819    4.333333\n",
      "1690    5.800000\n",
      "577     8.500000\n",
      "12447  16.500000\n",
      "11911  16.500000\n",
      "2288    4.117647\n",
      "12275   8.833333\n",
      "4341    6.166667\n",
      "1788    5.000000\n",
      "2218    9.555556\n",
      "5391    5.705882\n",
      "13048   7.000000\n",
      "\n",
      "[13452 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "y_mlp = gs_MLP.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482158786797503"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev, y_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_MLP_clf__activation</th>\n",
       "      <th>param_MLP_clf__hidden_layer_sizes</th>\n",
       "      <th>param_MLP_clf__max_iter</th>\n",
       "      <th>param_MLP_clf__momentum</th>\n",
       "      <th>param_MLP_clf__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277.731456</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>1.302234</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373.916279</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>1.333323</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.648922</td>\n",
       "      <td>0.673115</td>\n",
       "      <td>0.661007</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>357.605429</td>\n",
       "      <td>0.338305</td>\n",
       "      <td>1.623696</td>\n",
       "      <td>0.034237</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>474.193237</td>\n",
       "      <td>0.280791</td>\n",
       "      <td>1.402589</td>\n",
       "      <td>0.120243</td>\n",
       "      <td>tanh</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...</td>\n",
       "      <td>0.597974</td>\n",
       "      <td>0.627356</td>\n",
       "      <td>0.612651</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264.683728</td>\n",
       "      <td>0.129365</td>\n",
       "      <td>1.416832</td>\n",
       "      <td>0.077422</td>\n",
       "      <td>relu</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>353.529966</td>\n",
       "      <td>0.124757</td>\n",
       "      <td>1.388335</td>\n",
       "      <td>0.100204</td>\n",
       "      <td>relu</td>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.668937</td>\n",
       "      <td>0.740422</td>\n",
       "      <td>0.704647</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>340.663349</td>\n",
       "      <td>0.152085</td>\n",
       "      <td>1.470408</td>\n",
       "      <td>0.085144</td>\n",
       "      <td>relu</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>0.351429</td>\n",
       "      <td>0.351128</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>547.581941</td>\n",
       "      <td>0.392927</td>\n",
       "      <td>1.741871</td>\n",
       "      <td>0.329597</td>\n",
       "      <td>relu</td>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'MLP_clf__activation': 'relu', 'MLP_clf__hidd...</td>\n",
       "      <td>0.600507</td>\n",
       "      <td>0.705021</td>\n",
       "      <td>0.652716</td>\n",
       "      <td>0.052257</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     277.731456      0.010334         1.302234        0.010232   \n",
       "1     373.916279      0.055184         1.333323        0.013040   \n",
       "2     357.605429      0.338305         1.623696        0.034237   \n",
       "3     474.193237      0.280791         1.402589        0.120243   \n",
       "4     264.683728      0.129365         1.416832        0.077422   \n",
       "5     353.529966      0.124757         1.388335        0.100204   \n",
       "6     340.663349      0.152085         1.470408        0.085144   \n",
       "7     547.581941      0.392927         1.741871        0.329597   \n",
       "\n",
       "  param_MLP_clf__activation param_MLP_clf__hidden_layer_sizes  \\\n",
       "0                      tanh                            (4, 3)   \n",
       "1                      tanh                            (4, 3)   \n",
       "2                      tanh                            (5, 3)   \n",
       "3                      tanh                            (5, 3)   \n",
       "4                      relu                            (4, 3)   \n",
       "5                      relu                            (4, 3)   \n",
       "6                      relu                            (5, 3)   \n",
       "7                      relu                            (5, 3)   \n",
       "\n",
       "  param_MLP_clf__max_iter param_MLP_clf__momentum param_MLP_clf__solver  \\\n",
       "0                      50                     0.9                   sgd   \n",
       "1                      50                     0.9                  adam   \n",
       "2                      50                     0.9                   sgd   \n",
       "3                      50                     0.9                  adam   \n",
       "4                      50                     0.9                   sgd   \n",
       "5                      50                     0.9                  adam   \n",
       "6                      50                     0.9                   sgd   \n",
       "7                      50                     0.9                  adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.350828   \n",
       "1  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.648922   \n",
       "2  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.350828   \n",
       "3  {'MLP_clf__activation': 'tanh', 'MLP_clf__hidd...           0.597974   \n",
       "4  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.350828   \n",
       "5  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.668937   \n",
       "6  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.350828   \n",
       "7  {'MLP_clf__activation': 'relu', 'MLP_clf__hidd...           0.600507   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.351429         0.351128        0.000300                5  \n",
       "1           0.673115         0.661007        0.012096                2  \n",
       "2           0.351429         0.351128        0.000300                5  \n",
       "3           0.627356         0.612651        0.014691                4  \n",
       "4           0.351429         0.351128        0.000300                5  \n",
       "5           0.740422         0.704647        0.035743                1  \n",
       "6           0.351429         0.351128        0.000300                5  \n",
       "7           0.705021         0.652716        0.052257                3  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame.from_dict(gs_MLP.cv_results_)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
