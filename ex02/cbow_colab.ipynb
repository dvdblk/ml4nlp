{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "ni-VCVXpLc2Q",
    "outputId": "616d4d10-9f60-4290-b68a-378c39ff40c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /Users/debora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Collecting ray\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/6d/6625e213e3e011c8d4ab870195972f7761ed600c2761316248db00164c61/ray-0.7.6-cp37-cp37m-macosx_10_6_intel.whl (50.6MB)\n",
      "\u001b[K     |████████████████████████████████| 50.6MB 296kB/s eta 0:00:01     |████████████████████            | 31.7MB 395kB/s eta 0:00:48\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting pytest\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/16/f6dec5178f5f4141e80dfc4812a9aba88f5f29ca881f174ab1851181d016/pytest-5.2.2-py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from ray) (3.0.1)\n",
      "Requirement already satisfied: click in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from ray) (7.0)\n",
      "Collecting funcsigs\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.0.0 in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from ray) (1.11.0)\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
      "Collecting pyyaml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 4.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting redis>=3.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ae/28613a62eea0d53d3db3147f8715f90da07667e99baeedf1010eb400f8c0/redis-3.3.11-py2.py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf>=3.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c6/a8b6a74ab1e165f0aaa673a46f5c895af8780976880c98934ae82060356d/protobuf-3.10.0-cp37-cp37m-macosx_10_9_intel.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from ray) (1.16.2)\n",
      "Collecting pluggy<1.0,>=0.12\n",
      "  Downloading https://files.pythonhosted.org/packages/92/c7/48439f7d5fd6bddb4c04b850bb862b42e3e2b98570040dfaf68aedd8114b/pluggy-0.13.0-py2.py3-none-any.whl\n",
      "Collecting more-itertools>=4.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/dc/3241eef99eb45f1def35cf93af35d1cf9ef4c0991792583b8f33ea41b092/more_itertools-7.2.0-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 4.6MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from pytest->ray) (19.1.0)\n",
      "Collecting packaging\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/94/9672c2d4b126e74c4496c6b3c58a8b51d6419267be9e70660ba23374c875/packaging-19.2-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata>=0.12; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/d2/40b3fa882147719744e6aa50ac39cf7a22a913cbcba86a0371176c425a3b/importlib_metadata-0.23-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/bc/394ad449851729244a97857ee14d7cba61ddb268dce3db538ba2f2ba1f0f/py-1.8.0-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 3.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting atomicwrites>=1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/52/90/6155aa926f43f2b2a22b01be7241be3bfd1ceaf7d0b3267213e8127d41f4/atomicwrites-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from pytest->ray) (0.1.7)\n",
      "Requirement already satisfied: setuptools in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from jsonschema->ray) (40.6.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from jsonschema->ray) (0.14.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/debora/Envs/nlppython/lib/python3.7/site-packages (from packaging->pytest->ray) (2.4.0)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/74/3d/1ee25a26411ba0401b43c6376d2316a71addcc72ef8690b101b4ea56d76a/zipp-0.6.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp37-cp37m-macosx_10_14_x86_64.whl size=44109 sha256=180e7d7f18d370d6668ba7b4fa68c26b23b858c0bc65ff9348c14efb5c6925de\n",
      "  Stored in directory: /Users/debora/Library/Caches/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: more-itertools, zipp, importlib-metadata, pluggy, packaging, py, atomicwrites, pytest, funcsigs, filelock, colorama, pyyaml, redis, protobuf, ray\n",
      "  Found existing installation: protobuf 3.7.1\n",
      "    Uninstalling protobuf-3.7.1:\n",
      "      Successfully uninstalled protobuf-3.7.1\n",
      "Successfully installed atomicwrites-1.3.0 colorama-0.4.1 filelock-3.0.12 funcsigs-1.0.2 importlib-metadata-0.23 more-itertools-7.2.0 packaging-19.2 pluggy-0.13.0 protobuf-3.10.0 py-1.8.0 pytest-5.2.2 pyyaml-5.1.2 ray-0.7.6 redis-3.3.11 zipp-0.6.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3; however, version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt\n",
    "!pip install ray filelock\n",
    "#!pip uninstall -y pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQTQQGF_TMcD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from ray import tune\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0kpYNBCH9Ud"
   },
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGc_4Q6MH9Ui"
   },
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCL74l8dH9Uk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, add_unk=True):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        \n",
    "        self._token_to_ids = {}\n",
    "        self._ids_to_token = {}\n",
    "        \n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(\"<UNK>\") \n",
    "\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = list(set(self.tokens))\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_ids:\n",
    "            index = self._token_to_ids[token]\n",
    "        else:\n",
    "            index = len(self._token_to_ids)\n",
    "            self._token_to_ids[token] = index\n",
    "            self._ids_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_ids.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_ids[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._ids_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._ids_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDntDARcH9Uo"
   },
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3rYv4KBH9Up"
   },
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "        \"\"\"\n",
    "        vocabulary = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            # add each context word (token) to the vocabulary\n",
    "            for token in row.context:\n",
    "                vocabulary.add_token(token)\n",
    "                \n",
    "            # add the target word as well\n",
    "            vocabulary.add_token(row.target)\n",
    "            \n",
    "        return cls(vocabulary)\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.lookup_token(w) for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbaOxkBkH9Us"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_yL4r-MH9Ut"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, context_size, cbow_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "        \"\"\"\n",
    "        self.context_size = context_size\n",
    "        # 98/1/1% split\n",
    "        self.train_df, self.val_df, self.test_df = \\\n",
    "          np.split(cbow_df, [int(.98*len(cbow_df)), int(.99*len(cbow_df))])\n",
    "\n",
    "        self._lookup_dict = {'train': self.train_df,\n",
    "                             'val': self.val_df,\n",
    "                             'test': self.test_df}\n",
    "\n",
    "        self.set_split()\n",
    "        self._vectorizer = Vectorizer.from_dataframe(self.train_df)\n",
    "\n",
    "    @classmethod\n",
    "    def load_and_create_dataset(cls, filepath, context_size, frac=1.0):\n",
    "        \"\"\"Load and preprocess the dataset\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): location of the dataset\n",
    "            context_size (int): size of the context before/after the target word\n",
    "            frac (float, optional): fraction of the data to use (default 1.0)\n",
    "        Returns:\n",
    "            an instance of ShakespeareDataset\n",
    "        \"\"\"\n",
    "        # load the file\n",
    "        lines = ShakespeareDataset._load_file(filepath)\n",
    "        # consider the fraction param and throw away the rest\n",
    "        lines = lines[:int(len(lines)*frac)]\n",
    "        \n",
    "        # Preprocess\n",
    "        tokens = ShakespeareDataset._preprocess_and_split_lines(lines)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        dataframe_data = ShakespeareDataset._create_context_data(\n",
    "            tokens, \n",
    "            context_size\n",
    "        )\n",
    "        cbow_df = pd.DataFrame(dataframe_data, columns=['context', 'target'])\n",
    "        \n",
    "        # Create an instance \n",
    "        return cls(context_size, cbow_df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_file(filepath):\n",
    "        \"\"\"Load the dataset file into lines\"\"\"\n",
    "        with open(filepath) as file:\n",
    "            lines = file.readlines()\n",
    "            file.close()\n",
    "            return lines\n",
    "    \n",
    "    @staticmethod\n",
    "    def _preprocess_and_split_lines(lines):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            lines (list): a list of lines of the dataset\n",
    "        Returns:\n",
    "            a list of tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        # Regex\n",
    "        lines = lines[134:164924] #these numbers are only valid for the full corpus\n",
    "        text = ''.join(lines)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        #tokens = text.split()\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_context_data(tokens, context_size):\n",
    "        data = []\n",
    "        for i in range(context_size, len(tokens) - context_size):\n",
    "            # Context before w_i\n",
    "            context_before_w = tokens[i - context_size: i]\n",
    "\n",
    "            # Context after w_i\n",
    "            context_after_w = tokens[i + 1: i + context_size + 1]\n",
    "\n",
    "            # Put them together\n",
    "            context_window = context_before_w + context_after_w\n",
    "\n",
    "            # Target = w_i\n",
    "            target = tokens[i]\n",
    "\n",
    "            # Append in the correct format\n",
    "            data.append([context_window, target])\n",
    "        return data\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_df = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = self._vectorizer.vectorize(row.context)\n",
    "        target_index = self._vectorizer.vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aDv7WUgH9Uv"
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlOh92XhH9Uw"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self._context_window_size = context_size * 2\n",
    "        \n",
    "        # Embedding/input layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons) \n",
    "\n",
    "        # Output layer \n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = self.embeddings(inputs).sum(dim=1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds) / self._context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h)\n",
    "         \n",
    "        # output\n",
    "        # also note that we don't compute softmax here because Cross Entropy is used as a loss function\n",
    "        out = F.relu(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7Z8Es9DH9U3"
   },
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPFevBWyP00y"
   },
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.epoch_index = 0\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.model_filename = filename\n",
    "\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"Handle the training state updates.\n",
    "\n",
    "        model (nn.Module): model to save\n",
    "        \"\"\"\n",
    "        # Save one model at least once\n",
    "        if self.epoch_index == 0:\n",
    "            #torch.save(model.state_dict(), self.model_filename)\n",
    "            pass\n",
    "\n",
    "        # Save model if performance improved\n",
    "        else:\n",
    "            loss_prev, loss_cur = self.val_loss[-2:]\n",
    "\n",
    "            # compare current loss with the previous one\n",
    "            if loss_cur <= loss_prev:\n",
    "              # save if needed\n",
    "              #torch.save(model.state_dict(), self.model_filename)\n",
    "              pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s5TYE7OmH9U9",
    "outputId": "fda277da-491f-41b8-f993-e9015e3eb200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    shakespeare_csv_filepath=\"shakespeare-corpus.txt\",\n",
    "    model_state_file=\"shakespeare_model.pth\",\n",
    "    # Model hyper parameters\n",
    "    context_size=2,\n",
    "    num_neurons=128,\n",
    "    embedding_dim=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=40,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    # Runtime options\n",
    "    cuda=True\n",
    ")\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "class CBOWTrainingRoutine:\n",
    "\n",
    "    def create_new_classifier(self, vocab_len, embedding_dim, context_size,\n",
    "                              nr_hidden_neurons, device, learning_rate,\n",
    "                              filename):\n",
    "      # Classifier\n",
    "      self.loss_func = nn.CrossEntropyLoss()\n",
    "      classifier = CBOW(\n",
    "          vocab_len, \n",
    "          embedding_dim, \n",
    "          context_size, \n",
    "          nr_hidden_neurons)\n",
    "      self.classifier = classifier.to(device)\n",
    "      self.optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "      self.train_state = TrainStae(filename)\n",
    "\n",
    "\n",
    "    def train(self, dataset, num_epochs):\n",
    "      for epoch_index in tqdm(range(num_epochs)):\n",
    "          self.train_state.epoch_index = epoch_index\n",
    "\n",
    "          # Iterate over training dataset\n",
    "\n",
    "          # setup: batch generator, set loss to 0, set train mode on\n",
    "\n",
    "          dataset.set_split('train')\n",
    "          batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            device=device)\n",
    "          running_loss = 0.0\n",
    "          self.classifier.train()\n",
    "\n",
    "          for batch_index, batch_dict in enumerate(batch_generator):\n",
    "              # the training routine is these 5 steps:\n",
    "\n",
    "              # --------------------------------------\n",
    "              # step 1. zero the gradients\n",
    "              self.optimizer.zero_grad()\n",
    "\n",
    "              # step 2. compute the output\n",
    "              y_pred = self.classifier(batch_dict['x_data'])\n",
    "\n",
    "              # step 3. compute the loss\n",
    "              loss = self.loss_func(y_pred, batch_dict['y_target'])\n",
    "              loss_t = loss.item()\n",
    "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "              # step 4. use loss to produce gradients\n",
    "              loss.backward()\n",
    "\n",
    "              # step 5. use optimizer to take gradient step\n",
    "              self.optimizer.step()\n",
    "              # -----------------------------------------\n",
    "\n",
    "          self.train_state.train_loss.append(running_loss)\n",
    "\n",
    "          # Iterate over val dataset\n",
    "\n",
    "          # setup: batch generator, set loss to 0; set eval mode on\n",
    "          dataset.set_split('val')\n",
    "          batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            device=device)\n",
    "          running_loss = 0.0\n",
    "          self.classifier.eval()\n",
    "\n",
    "          for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "              # compute the output\n",
    "              y_pred =  self.classifier(batch_dict['x_data'])\n",
    "\n",
    "              # compute the loss\n",
    "              loss = self.loss_func(y_pred, batch_dict['y_target'])\n",
    "              loss_t = loss.item()\n",
    "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "          self.train_state.val_loss.append(running_loss)\n",
    "\n",
    "          self.train_state.update(model=self.classifier)\n",
    "      return train_state\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6DcH0JaHjQw"
   },
   "outputs": [],
   "source": [
    "class CBOWTrainable(tune.Trainable):\n",
    "\n",
    "    def _save(self, checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, args.model_state_file)\n",
    "        torch.save(self.classifier.state_dict(), checkpoint_path)\n",
    "        return checkpoint_path\n",
    "\n",
    "    def _restore(self, checkpoint_path):\n",
    "        self.classifier.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def _setup(self, config):\n",
    "      self.dataset = config.get(\"dataset\")\n",
    "      vectorizer = dataset.get_vectorizer()\n",
    "      training_routine = config.get(\"training_routine\")\n",
    "      training_routine.create_new_classifier(\n",
    "          len(vectorizer.vocab), args.embedding_dim, \n",
    "          self.dataset.context_size, config.get(\"nr_hidden_neurons\"), \n",
    "          args.device, config.get(\"lr\"), args.model_state_file\n",
    "      )\n",
    "      self.training_routine = training_routine\n",
    "      \n",
    "\n",
    "    def _train(self):\n",
    "      train_state = self.training_routine.train(\n",
    "          self.dataset, \n",
    "          args.num_epochs,\n",
    "          args.batch_size,\n",
    "          args.device,\n",
    "      )\n",
    "      return { 'loss': train_state.val_loss[-1] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhcKzv2fGK0l"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = ShakespeareDataset.load_and_create_dataset(\n",
    "    args.shakespeare_csv_filepath,\n",
    "    args.context_size,\n",
    "    0.2\n",
    ")\n",
    "\n",
    "training_routine = CBOWTrainingRoutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_JsVzIt7F2A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 14:21:32,462\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-11-02 14:21:32,468\tINFO resource_spec.py:205 -- Starting Ray with 1.46 GiB memory available for workers and up to 0.74 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/1.46 GiB heap, 0.0/0.49 GiB objects\n",
      "Memory usage on this node: 5.8/8.0 GiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    CBOWTrainable, \n",
    "    config={\n",
    "      \"lr\": tune.grid_search([0.001, 0.01]),\n",
    "      \"nr_hidden_neurons\": tune.grid_search([10, 30, 50, 100]),\n",
    "      \"training_routine\": training_routine,\n",
    "      \"dataset\": dataset\n",
    "    },\n",
    "    local_dir=\".\",\n",
    "    resources_per_trial={\n",
    "        \"gpu\": int(args.cuda),\n",
    "        \"cpu\": 2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjhH-ovXH9VA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8Mq0Mf5H9VC"
   },
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPhtDBAJB8Rk"
   },
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5Va332XH9VC"
   },
   "outputs": [],
   "source": [
    "# Part2 supplied function\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = classifier.embeddings\n",
    "    test_vocab = dataset.get_vectorizer().vocab\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = test_vocab.lookup_token(word)\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long).to(args.device)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long).to(args.device)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.lookup_index(j), float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]\n",
    "\n",
    "get_closest_word('desire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ga2LBQWH9VE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_examples = len(data)\n",
    "pred_sum = 0 # softmax check\n",
    "acc_sum = 0 # accuracy\n",
    "\n",
    "for i in range(nr_examples):\n",
    "    ids = vectorizer.vectorize(data[i][0])\n",
    "    target = test_vocab.tok_to_ids[data[i][1]]\n",
    "    pred = model(ids) # prediction\n",
    "    pred_sum += pred.squeeze().sum().item() \n",
    "    \n",
    "    _, pred_index = pred.max(dim=1) # prediction index\n",
    "    n_correct = torch.eq(pred_index, target)\n",
    "    acc_sum += n_correct.item()\n",
    "    \n",
    "    print(\"Prediction: \" + str(pred_index.item()), \"| Target: \" + str(target))\n",
    "    \n",
    "print(acc_sum / nr_examples)\n",
    "print(pred_sum / nr_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzHWb514H9VK"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J3uP5ASH9VK"
   },
   "outputs": [],
   "source": [
    "stringo = \"here is an [_exit_]\"\n",
    "stringo = re.sub('(\\[_).*(_\\])', '', stringo)\n",
    "print(stringo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0Hx59HFH9VN"
   },
   "outputs": [],
   "source": [
    "#finis is 164924\n",
    "#beginngin is line 134 --> just keep what's in between those lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tM0tVkRH9VQ"
   },
   "outputs": [],
   "source": [
    "filename = 'shakespeare-corpus.txt'\n",
    "file = open(filename)\n",
    "lines = file.readlines()\n",
    "lines = lines[134:164924]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmo_YhzQH9VR"
   },
   "outputs": [],
   "source": [
    "def mytext(lines):\n",
    "    corpus = ''\n",
    "    for line in lines:\n",
    "        text = re.sub(r'\\d+', '', line)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        corpus += text\n",
    "    return corpus\n",
    "\n",
    "%time len(mytext(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QB0kV7wJH9VS"
   },
   "outputs": [],
   "source": [
    "def mytext2(lines):\n",
    "    text = ''.join(lines)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('SCENE \\S', '', text)\n",
    "    text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "    text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "%time len(mytext2(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGJthXarH9VS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJdHFnv9JQfm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cbow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
