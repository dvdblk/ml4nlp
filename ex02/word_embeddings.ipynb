{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DQTQQGF_TMcD",
    "outputId": "860d0c42-0384-4202-ef8b-b8370ef7d46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ceac470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, filepath):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.tokens = self.nltk_tokenize()\n",
    "        self.tok_to_ids, self.ids_to_tok = self.make_dicts()\n",
    "        self.nr_unique_tokens = len(self.vocabulary_set())\n",
    "    \n",
    "    def readfile(self):\n",
    "        \"\"\"this function opens the file and returns the text in a string\"\"\"\n",
    "        file = open(self.filepath)\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "    \n",
    "    def nltk_tokenize(self):\n",
    "        \"\"\"this function tokenizes the text and returns a list of tokens as strings\"\"\"\n",
    "        text = self.readfile()\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = self.vocabulary_set()\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tok_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.tok_to_ids[w] for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'test_corpus.txt'\n",
    "test_vocab = Vocabulary(filepath)\n",
    "vectorizer = Vectorizer(test_vocab)\n",
    "\n",
    "# Size of the context windows, 2 and 5 are supposed to be used in ex02...\n",
    "# range \\in [2, 1/2 * document_length - 1]\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# let's stick with this notation for now ;)\n",
    "CONTEXT_WINDOW_SIZE = CONTEXT_SIZE * 2\n",
    "\n",
    "\n",
    "# Data creation - get context around the target word\n",
    "data = []\n",
    "tokens = test_vocab.tokens\n",
    "for i in range(CONTEXT_SIZE, len(tokens) - CONTEXT_SIZE):\n",
    "    # Context before w_i\n",
    "    context_before_w = tokens[i - CONTEXT_SIZE: i]\n",
    "    \n",
    "    # Context after w_i\n",
    "    context_after_w = tokens[i + 1: i + CONTEXT_SIZE + 1]\n",
    "    \n",
    "    # Put them together\n",
    "    context_window = context_before_w + context_after_w\n",
    "    \n",
    "    # Target = w_i\n",
    "    target = tokens[i]\n",
    "    \n",
    "    # Append in the correct format\n",
    "    data.append((context_window, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # note: this probably doesn't deal with 'UNK' words\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons)  \n",
    "        \n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = sum(self.embeddings(inputs))\n",
    "\n",
    "        # shape = (1, EMBEDDING_DIM)\n",
    "        # -1 param in view() ... \"the actual value for this dimension will be inferred so that the number of elements in the view matches the original number of elements.\"\n",
    "        embeds_2D = embeds.view(1, -1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds_2D) / self.context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h) \n",
    "         \n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.5256, -0.7502, -0.6540,  ..., -0.6298, -0.9274,  0.5451],\n",
      "        [ 0.0663, -0.4370,  0.7626,  ...,  1.1899,  0.8165, -0.9135],\n",
      "        [ 1.3851, -0.8138, -0.9276,  ...,  0.6419,  0.4730, -0.4286],\n",
      "        ...,\n",
      "        [-2.6061,  1.6771,  0.6073,  ...,  0.2068,  1.5356,  2.0230],\n",
      "        [-0.0422,  0.5282,  1.4127,  ...,  1.0123,  1.0741, -0.8674],\n",
      "        [-1.0755, -1.6713,  1.0103,  ..., -0.1815, -0.9565,  0.5544]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2140.3685624599457, 1856.9114190340042, 1549.4629547819495, 1119.8741447571665, 600.4194824802689, 229.9869882969033, 100.09525637446313, 55.6612077549907, 26.30374109443801, 19.925122829203247, 11.694381759784847, 10.055085795338528, 9.004289690579753, 8.201048040014939, 7.58014596027715, 7.076458833071229, 6.65117995158289, 6.300688938697931, 5.998086726755446, 5.744755854947016, 5.513304552205227, 5.307536289304608, 5.128261977941293, 4.965124934028154, 4.819614729301975, 4.685921886021788, 4.565508244000284, 4.4598745606554075, 4.352509277080117, 4.267838650705471, 4.1762281610572245, 4.089543600316119, 4.025071966489577, 3.9442779442451865, 3.876551443583139, 3.8198289013398607, 3.755192917189106, 3.69926191669083, 3.6515934343624394, 3.6071754065135337, 3.5512068579139395, 3.504560904696291, 3.4671490395849105, 3.4223116729690446, 3.3828150984516014, 3.347106567221772, 3.3198804429687243, 3.280975121398342, 3.244982886036496, 3.212801146619313, 3.187191225942115, 3.155993433329513, 3.127734831750331, 3.106577825966724, 3.079521615783733, 3.0521367275139255, 3.0287598346737923, 3.0054981056855468, 2.982240887356511, 2.962782784317369, 2.940770645136354, 2.924783442227408, 2.918033377734673, 2.8852916515750167, 2.8650604029253373, 2.8471828343435845, 2.829589518585408, 2.812015844629741, 2.7973521201329277, 2.7962814502425317, 2.7678017951939182, 2.75095406260607, 2.735395454029913, 2.7221936583115394, 2.7079968983604203, 2.694021434512365, 2.681009279399518, 2.6800523607108744, 2.6656157789578856, 2.645089850804993, 2.63222533942087, 2.6200832576267885, 2.609233393893078, 2.597826354750282, 2.588105643373524, 2.5845008310161575, 2.5696019960978447, 2.5573336681750334, 2.5468110745423473, 2.537286500175469, 2.527121095919938, 2.5177037756088794, 2.5094584278681396, 2.5064021878895346, 2.4924354977531493, 2.4848868735027736, 2.4743810023289825, 2.4662954007119424, 2.4583810158903816, 2.4502605525624404]\n",
      "Parameter containing:\n",
      "tensor([[-1.7529, -0.8788, -0.6046,  ..., -0.7079, -1.0361,  0.5992],\n",
      "        [ 0.1232, -0.4047,  0.8189,  ...,  1.2363,  0.7665, -0.9457],\n",
      "        [ 1.3627, -0.7806, -0.8728,  ...,  0.7065,  0.4331, -0.4079],\n",
      "        ...,\n",
      "        [-2.6290,  1.7452,  0.6468,  ...,  0.2415,  1.5631,  2.0788],\n",
      "        [-0.0667,  0.4544,  1.3919,  ...,  1.0445,  1.0756, -0.8341],\n",
      "        [-1.0701, -1.6514,  1.0929,  ..., -0.1559, -0.9315,  0.6146]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_ITERATIONS = 100\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss() # negative log likelihood loss\n",
    "model = CBOW(test_vocab.nr_unique_tokens, EMBEDDING_DIM, CONTEXT_WINDOW_SIZE)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(model.embeddings.weight)\n",
    "\n",
    "for epoch in tqdm(range(NUM_ITERATIONS)):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        # Step1. Create input vector \n",
    "        context_vectors = vectorizer.vectorize(context)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_vectors)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        target = torch.tensor(vectorizer.vocab.tok_to_ids[target], dtype=torch.long).view(1)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print(losses)\n",
    "print(model.embeddings.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 7.565771102905273),\n",
       " ('were', 7.587931156158447),\n",
       " ('1', 7.670988082885742),\n",
       " ('Or', 7.82880163192749),\n",
       " ('uneared', 8.229613304138184)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = model.embeddings\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = test_vocab.tok_to_ids[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long) \n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.ids_to_tok[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1]) \n",
    "    return word_distance[:topn]\n",
    "\n",
    "get_closest_word('that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of word_embeddings_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
