{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "ni-VCVXpLc2Q",
    "outputId": "f442b00b-da26-467f-8108-87986f5d674a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /Users/debora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQTQQGF_TMcD"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0kpYNBCH9Ud"
   },
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGc_4Q6MH9Ui"
   },
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCL74l8dH9Uk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, add_unk=True):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        \n",
    "        self._token_to_ids = {}\n",
    "        self._ids_to_token = {}\n",
    "        \n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(\"<UNK>\") \n",
    "\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = list(set(self.tokens))\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_ids:\n",
    "            index = self._token_to_ids[token]\n",
    "        else:\n",
    "            index = len(self._token_to_ids)\n",
    "            self._token_to_ids[token] = index\n",
    "            self._ids_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_ids.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_ids[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._ids_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._ids_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDntDARcH9Uo"
   },
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3rYv4KBH9Up"
   },
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "        \"\"\"\n",
    "        vocabulary = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            # add each context word (token) to the vocabulary\n",
    "            for token in row.context:\n",
    "                vocabulary.add_token(token)\n",
    "                \n",
    "            # add the target word as well\n",
    "            vocabulary.add_token(row.target)\n",
    "            \n",
    "        return cls(vocabulary)\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.lookup_token(w) for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbaOxkBkH9Us"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_yL4r-MH9Ut"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "        \"\"\"\n",
    "        # 98/1/1% split\n",
    "        self.train_df, self.val_df, self.test_df = \\\n",
    "          np.split(cbow_df, [int(.98*len(cbow_df)), int(.99*len(cbow_df))])\n",
    "\n",
    "        self._lookup_dict = {'train': self.train_df,\n",
    "                             'val': self.val_df,\n",
    "                             'test': self.test_df}\n",
    "\n",
    "        self.set_split()\n",
    "        self._vectorizer = Vectorizer.from_dataframe(self.train_df)\n",
    "\n",
    "    @classmethod\n",
    "    def load_and_create_dataset(cls, filepath, context_size, frac=1.0):\n",
    "        \"\"\"Load and preprocess the dataset\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): location of the dataset\n",
    "            context_size (int): size of the context before/after the target word\n",
    "            frac (float, optional): fraction of the data to use (default 1.0)\n",
    "        Returns:\n",
    "            an instance of ShakespeareDataset\n",
    "        \"\"\"\n",
    "        # load the file\n",
    "        lines = ShakespeareDataset._load_file(filepath)\n",
    "        # consider the fraction param and throw away the rest\n",
    "        lines = lines[:int(len(lines)*frac)]\n",
    "        \n",
    "        # Preprocess\n",
    "        tokens = ShakespeareDataset._preprocess_and_split_lines(lines)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        dataframe_data = ShakespeareDataset._create_context_data(\n",
    "            tokens, \n",
    "            context_size\n",
    "        )\n",
    "        cbow_df = pd.DataFrame(dataframe_data, columns=['context', 'target'])\n",
    "        \n",
    "        # Create an instance \n",
    "        return cls(cbow_df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_file(filepath):\n",
    "        \"\"\"Load the dataset file into lines\"\"\"\n",
    "        with open(filepath) as file:\n",
    "            lines = file.readlines()\n",
    "            file.close()\n",
    "            return lines\n",
    "    \n",
    "    @staticmethod\n",
    "    def _preprocess_and_split_lines(lines):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            lines (list): a list of lines of the dataset\n",
    "        Returns:\n",
    "            a list of tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        # Regex\n",
    "        lines = lines[134:164924] #these numbers are only valid for the full corpus\n",
    "        text = ''.join(lines)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+â€”/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        #tokens = text.split()\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_context_data(tokens, context_size):\n",
    "        data = []\n",
    "        for i in range(context_size, len(tokens) - context_size):\n",
    "            # Context before w_i\n",
    "            context_before_w = tokens[i - context_size: i]\n",
    "\n",
    "            # Context after w_i\n",
    "            context_after_w = tokens[i + 1: i + context_size + 1]\n",
    "\n",
    "            # Put them together\n",
    "            context_window = context_before_w + context_after_w\n",
    "\n",
    "            # Target = w_i\n",
    "            target = tokens[i]\n",
    "\n",
    "            # Append in the correct format\n",
    "            data.append([context_window, target])\n",
    "        return data\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_df = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = self._vectorizer.vectorize(row.context)\n",
    "        target_index = self._vectorizer.vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aDv7WUgH9Uv"
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlOh92XhH9Uw"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self._context_window_size = context_size * 2\n",
    "        \n",
    "        # Embedding/input layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons) \n",
    "\n",
    "        # Output layer \n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = self.embeddings(inputs).sum(dim=1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds) / self._context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h)\n",
    "         \n",
    "        # output\n",
    "        # also note that we don't compute softmax here because Cross Entropy is used as a loss function\n",
    "        out = F.relu(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7Z8Es9DH9U3"
   },
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPFevBWyP00y"
   },
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.epoch_index = 0\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.model_filename = filename\n",
    "\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"Handle the training state updates.\n",
    "\n",
    "        model (nn.Module): model to save\n",
    "        \"\"\"\n",
    "        # Save one model at least once\n",
    "        if self.epoch_index == 0:\n",
    "            torch.save(model.state_dict(), self.model_filename)\n",
    "\n",
    "        # Save model if performance improved\n",
    "        else:\n",
    "            loss_prev, loss_cur = self.val_loss[-2:]\n",
    "\n",
    "            # compare current loss with the previous one\n",
    "            if loss_cur <= loss_prev:\n",
    "              # save if needed\n",
    "              torch.save(model.state_dict(), self.model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s5TYE7OmH9U9",
    "outputId": "1ac31dd9-7bfa-44cd-bd9a-e5774b22fbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    shakespeare_csv_filepath=\"shakespeare-corpus.txt\",\n",
    "    model_state_file=\"shakespeare_model.pth\",\n",
    "    model_state_dir=\"models/\",\n",
    "    # Model hyper parameters\n",
    "    context_size=2,\n",
    "    num_neurons=128,\n",
    "    embedding_dim=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    # Runtime options\n",
    "    cuda=True\n",
    ")\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "class CBOWTrainingRoutine:\n",
    "\n",
    "    def create_new_classifier(self, vocab_len, embedding_dim, context_size,\n",
    "                              nr_hidden_neurons, device, learning_rate,\n",
    "                              filedir, filepath):\n",
    "      # Classifier\n",
    "      self.loss_func = nn.CrossEntropyLoss()\n",
    "      classifier = CBOW(\n",
    "          vocab_len, \n",
    "          embedding_dim, \n",
    "          context_size, \n",
    "          nr_hidden_neurons)\n",
    "      self.classifier = classifier.to(device)\n",
    "      self.optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "      filename = str(nr_hidden_neurons) + \"_\" + str(learning_rate) + \"_\" + filepath\n",
    "      self.train_state = TrainState(filedir + filename)\n",
    "\n",
    "\n",
    "    def train(self, dataset, num_epochs, batch_size, device):\n",
    "      for epoch_index in tqdm(range(num_epochs)):\n",
    "          self.train_state.epoch_index = epoch_index\n",
    "\n",
    "          # Iterate over training dataset\n",
    "\n",
    "          # setup: batch generator, set loss to 0, set train mode on\n",
    "\n",
    "          dataset.set_split('train')\n",
    "          batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            device=device)\n",
    "          running_loss = 0.0\n",
    "          self.classifier.train()\n",
    "\n",
    "          for batch_index, batch_dict in enumerate(batch_generator):\n",
    "              # the training routine is these 5 steps:\n",
    "\n",
    "              # --------------------------------------\n",
    "              # step 1. zero the gradients\n",
    "              self.optimizer.zero_grad()\n",
    "\n",
    "              # step 2. compute the output\n",
    "              y_pred = self.classifier(batch_dict['x_data'])\n",
    "\n",
    "              # step 3. compute the loss\n",
    "              loss = self.loss_func(y_pred, batch_dict['y_target'])\n",
    "              loss_t = loss.item()\n",
    "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "              # step 4. use loss to produce gradients\n",
    "              loss.backward()\n",
    "\n",
    "              # step 5. use optimizer to take gradient step\n",
    "              self.optimizer.step()\n",
    "              # -----------------------------------------\n",
    "\n",
    "          self.train_state.train_loss.append(running_loss)\n",
    "\n",
    "          # Iterate over val dataset\n",
    "\n",
    "          # setup: batch generator, set loss to 0; set eval mode on\n",
    "          dataset.set_split('val')\n",
    "          batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            device=device)\n",
    "          running_loss = 0.0\n",
    "          self.classifier.eval()\n",
    "\n",
    "          for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "              # compute the output\n",
    "              y_pred =  self.classifier(batch_dict['x_data'])\n",
    "\n",
    "              # compute the loss\n",
    "              loss = self.loss_func(y_pred, batch_dict['y_target'])\n",
    "              loss_t = loss.item()\n",
    "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "          self.train_state.val_loss.append(running_loss)\n",
    "\n",
    "          self.train_state.update(model=self.classifier)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhcKzv2fGK0l"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = ShakespeareDataset.load_and_create_dataset(\n",
    "    args.shakespeare_csv_filepath,\n",
    "    args.context_size,\n",
    "    0.005\n",
    ")\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "training_routine = CBOWTrainingRoutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "4_JsVzIt7F2A",
    "outputId": "a00c2b1f-7195-40c2-ad4f-5aa555471af9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db71524f53b4915937c67627093abf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bef21aa26f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f6b9adc74084>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, num_epochs, batch_size, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m               \u001b[0;31m# the training routine is these 5 steps:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-201e1f432509>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[0;34m(dataset, batch_size, shuffle, drop_last, device)\u001b[0m\n\u001b[1;32m    144\u001b[0m                             shuffle=shuffle, drop_last=drop_last)\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mout_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-201e1f432509>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         return {'x_data': context_vector,\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5066\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5067\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4348\u001b[0m         \u001b[0;31m# use this, e.g. DatetimeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4349\u001b[0m         \u001b[0;31m# Things like `Series._get_value` (via .at) pass the EA directly here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4350\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mExtensionArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4352\u001b[0m             \u001b[0;31m# GH 20882, 21257\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mrepr\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_formatting_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minternal_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minternal_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformatting_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/nlppython/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time, os\n",
    "\n",
    "grid_search_params = {\n",
    "      \"lr\": [0.001],\n",
    "      \"nr_hidden_neurons\": [2, 15]\n",
    "}\n",
    "\n",
    "\n",
    "values = [lists for _, lists in grid_search_params.items()]\n",
    "classifiers = []\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_gridsearch_dir = args.model_state_dir + \"gridsearch/\"\n",
    "model_dir = model_gridsearch_dir + timestr + \"_\"  + str(args.num_epochs) + \"/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "for lr, nr_hidden in itertools.product(*values):\n",
    "    training_routine.create_new_classifier(\n",
    "      len(vectorizer.vocab), args.embedding_dim, \n",
    "      args.context_size, nr_hidden, \n",
    "      args.device, lr, model_dir, args.model_state_file\n",
    "    )\n",
    "\n",
    "    training_routine.train(\n",
    "      dataset, \n",
    "      args.num_epochs,\n",
    "      args.batch_size,\n",
    "      args.device,\n",
    "    )\n",
    "\n",
    "    classifiers.append((training_routine.classifier, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjhH-ovXH9VA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8Mq0Mf5H9VC"
   },
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of pretrained models (classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(CBOW(\n",
       "    (embeddings): Embedding(1318, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
       "    (linear2): Linear(in_features=2, out_features=1318, bias=True)\n",
       "  ), 0.001)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell loads the last performd gridsearch classifiers\n",
    "\n",
    "import os\n",
    "\n",
    "grid_search_directories = os.listdir(model_gridsearch_dir)\n",
    "grid_search_directories.sort()\n",
    "last_grid_search_dir = grid_search_directories[-1]\n",
    "target_dir = model_gridsearch_dir + last_grid_search_dir\n",
    "\n",
    "classifiers_loaded = []\n",
    "\n",
    "for file in os.listdir(target_dir):\n",
    "    if file.endswith(\".pth\"):\n",
    "        # get the number of neurons from filename\n",
    "        str_hidden, str_lr, *rest = file.split(\"_\")\n",
    "        # init the classifier\n",
    "        classifier = CBOW(len(vectorizer.vocab), args.embedding_dim, args.context_size, int(str_hidden))\n",
    "        # load the weights / embeddings\n",
    "        classifier.load_state_dict(torch.load(os.path.join(target_dir, file)))\n",
    "        # set to eval mode\n",
    "        classifier.eval()\n",
    "        # add to the list of loaded classifiers\n",
    "        classifiers_loaded.append((classifier, float(str_lr)))\n",
    "        \n",
    "        \n",
    "# classifiers from the last grid_search\n",
    "classifiers_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5Va332XH9VC"
   },
   "outputs": [],
   "source": [
    "def get_closest_word_pwd(classifier, word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = classifier.embeddings\n",
    "    test_vocab = dataset.get_vectorizer().vocab\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = test_vocab.lookup_token(word)\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long).to(args.device)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long).to(args.device)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.lookup_index(j), float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]\n",
    "\n",
    "def get_closest_word_cs(classifier, word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = classifier.embeddings\n",
    "    test_vocab = dataset.get_vectorizer().vocab\n",
    "    pdist = nn.CosineSimilarity()\n",
    "    i = test_vocab.lookup_token(word)\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long).to(args.device)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long).to(args.device)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.lookup_index(j), float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[::-1][:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(CBOW(\n",
      "  (embeddings): Embedding(1318, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=1318, bias=True)\n",
      "), 0.001)]\n"
     ]
    }
   ],
   "source": [
    "print(classifiers_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(1318, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=1318, bias=True)\n",
      ")\n",
      "\n",
      "===Pairwise Distance (lower better)===\n",
      "...[6.75] - canopy\n",
      "...[6.93] - object\n",
      "...[7.29] - pen\n",
      "...[7.43] - youthful\n",
      "...[7.44] - brood\n",
      "===Cosine Similarity (higher better)===\n",
      "...[0.46] - pen\n",
      "...[0.45] - canopy\n",
      "...[0.42] - youthful\n",
      "...[0.39] - reigns\n",
      "...[0.38] - pearl\n"
     ]
    }
   ],
   "source": [
    "target_classifiers = classifiers_loaded if not classifiers else classifiers\n",
    "\n",
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    Pretty print embedding results.\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "for classifier, lr in target_classifiers:\n",
    "    word = 'king'\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Classifier (LR: \" + str(lr) + \"): \" + str(classifier) + \"\\n\")\n",
    "    print(\"===Pairwise Distance (lower better)===\")\n",
    "    pretty_print(get_closest_word_pwd(classifier, word))\n",
    "    print(\"===Cosine Similarity (higher better)===\")\n",
    "    pretty_print(get_closest_word_cs(classifier, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ga2LBQWH9VE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def accuracy_check(classifier):\n",
    "    pred_sum = 0 # softmax check\n",
    "    acc_sum = 0 # accuracy\n",
    "    \n",
    "        \n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        print(batch_index)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        print(acc_t / args.batch_size)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(1318, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=1318, bias=True)\n",
      ")\n",
      "\n",
      "0\n",
      "0.29296875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier, lr in target_classifiers:\n",
    "    print(\"Classifier (LR: \" + str(lr) + \"): \" + str(classifier) + \"\\n\")\n",
    "    accuracy_check(classifier)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzHWb514H9VK"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J3uP5ASH9VK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is an \n"
     ]
    }
   ],
   "source": [
    "stringo = \"here is an [_exit_]\"\n",
    "stringo = re.sub('(\\[_).*(_\\])', '', stringo)\n",
    "print(stringo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0Hx59HFH9VN"
   },
   "outputs": [],
   "source": [
    "#finis is 164924\n",
    "#beginngin is line 134 --> just keep what's in between those lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tM0tVkRH9VQ"
   },
   "outputs": [],
   "source": [
    "filename = 'shakespeare-corpus.txt'\n",
    "file = open(filename)\n",
    "lines = file.readlines()\n",
    "lines = lines[134:164924]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmo_YhzQH9VR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 15.5 ms, total: 1.03 s\n",
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5521081"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mytext(lines):\n",
    "    corpus = ''\n",
    "    for line in lines:\n",
    "        text = re.sub(r'\\d+', '', line)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+â€”/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        corpus += text\n",
    "    return corpus\n",
    "\n",
    "%time len(mytext(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QB0kV7wJH9VS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 295 ms, sys: 29.2 ms, total: 324 ms\n",
      "Wall time: 379 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5521081"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mytext2(lines):\n",
    "    text = ''.join(lines)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('SCENE \\S', '', text)\n",
    "    text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "    text = re.sub(r'[\\\\[#$%*+â€”/<=>?{}|~@]+_', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "%time len(mytext2(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGJthXarH9VS"
   },
   "outputs": [],
   "source": [
    "#MOST COMMON WORDS\n",
    "filename = 'shakespeare-corpus.txt'\n",
    "file = open(filename)\n",
    "lines = file.readlines()\n",
    "lines = lines[134:164924] #these numbers are only valid for the full corpus\n",
    "text = ''.join(lines)\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "text = re.sub('SCENE \\S', '', text)\n",
    "text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "text = re.sub(r'[\\\\[#$%*+â€”/<=>?{}|~@]+_', '', text)\n",
    "text = text.lower()\n",
    "        \n",
    "# Tokenize\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "#tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJdHFnv9JQfm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('must', 1638)\n",
      "('had', 1555)\n",
      "('see', 1554)\n",
      "('why', 1539)\n",
      "('such', 1528)\n",
      "(']', 1499)\n",
      "('where', 1472)\n",
      "('out', 1452)\n",
      "('some', 1434)\n",
      "('who', 1429)\n",
      "('give', 1424)\n",
      "('these', 1411)\n",
      "('first', 1353)\n",
      "('[', 1342)\n",
      "('ll', 1341)\n",
      "('too', 1338)\n",
      "('take', 1288)\n",
      "('mine', 1261)\n",
      "('most', 1237)\n",
      "('speak', 1207)\n",
      "('duke', 1157)\n",
      "('time', 1156)\n",
      "(\"'ll\", 1142)\n",
      "('up', 1139)\n",
      "('never', 1135)\n",
      "('tell', 1116)\n",
      "('heart', 1113)\n",
      "('father', 1105)\n",
      "('much', 1091)\n",
      "('doth', 1081)\n",
      "('think', 1073)\n",
      "('nor', 1057)\n",
      "('th', 1043)\n",
      "('queen', 1023)\n",
      "('men', 1012)\n",
      "('lady', 994)\n",
      "('art', 993)\n",
      "('great', 966)\n",
      "('look', 958)\n",
      "('death', 957)\n",
      "('life', 946)\n",
      "('before', 938)\n",
      "('hear', 914)\n",
      "('god', 909)\n",
      "('away', 903)\n",
      "('made', 899)\n",
      "('hand', 898)\n",
      "('master', 861)\n",
      "('sweet', 856)\n",
      "('very', 851)\n",
      "('true', 849)\n",
      "('fair', 840)\n",
      "('thus', 830)\n",
      "(\"'t\", 820)\n",
      "('tis', 816)\n",
      "('own', 809)\n",
      "('prince', 805)\n",
      "('eyes', 800)\n",
      "('day', 797)\n",
      "('again', 795)\n",
      "('pray', 793)\n",
      "('ay', 773)\n",
      "('call', 762)\n",
      "('any', 761)\n",
      "('two', 755)\n",
      "('being', 749)\n",
      "('honour', 747)\n",
      "('old', 743)\n",
      "('other', 743)\n",
      "('night', 741)\n",
      "('been', 730)\n",
      "('gloucester', 729)\n",
      "('world', 727)\n",
      "('fear', 727)\n",
      "('done', 713)\n",
      "('son', 711)\n",
      "('name', 706)\n",
      "('both', 703)\n",
      "('leave', 699)\n",
      "('heaven', 699)\n",
      "('whose', 695)\n",
      "('t', 693)\n",
      "('till', 690)\n",
      "('blood', 686)\n",
      "('though', 685)\n",
      "('could', 681)\n",
      "('poor', 680)\n",
      "('nothing', 675)\n",
      "('into', 671)\n",
      "('noble', 669)\n",
      "('down', 656)\n",
      "('ever', 655)\n",
      "('therefore', 653)\n",
      "('henry', 649)\n",
      "('comes', 647)\n",
      "('brother', 647)\n",
      "('even', 633)\n",
      "('better', 632)\n",
      "('still', 630)\n",
      "('exeunt', 626)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(200)[100:]\n",
    "\n",
    "for i in most_frequent(tokens):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbs:is, be, have, will, 'd, 's, do, shall, come, would, was, let, would\n",
    "#nouns: lord, king, sir, love, man, time, heart, father\n",
    "#adjectives: good, first, great, sweet, own, old, other"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cbow_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
