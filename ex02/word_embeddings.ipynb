{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "ni-VCVXpLc2Q",
    "outputId": "f442b00b-da26-467f-8108-87986f5d674a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /Users/dvdblk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQTQQGF_TMcD"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0kpYNBCH9Ud"
   },
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGc_4Q6MH9Ui"
   },
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCL74l8dH9Uk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, add_unk=True):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        \n",
    "        self._token_to_ids = {}\n",
    "        self._ids_to_token = {}\n",
    "        \n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(\"<UNK>\") \n",
    "\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = list(set(self.tokens))\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_ids:\n",
    "            index = self._token_to_ids[token]\n",
    "        else:\n",
    "            index = len(self._token_to_ids)\n",
    "            self._token_to_ids[token] = index\n",
    "            self._ids_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_ids.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_ids[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._ids_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._ids_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDntDARcH9Uo"
   },
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3rYv4KBH9Up"
   },
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "        \"\"\"\n",
    "        vocabulary = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            # add each context word (token) to the vocabulary\n",
    "            for token in row.context:\n",
    "                vocabulary.add_token(token)\n",
    "                \n",
    "            # add the target word as well\n",
    "            vocabulary.add_token(row.target)\n",
    "            \n",
    "        return cls(vocabulary)\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.lookup_token(w) for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbaOxkBkH9Us"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_yL4r-MH9Ut"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "        \"\"\"\n",
    "        # 98/1/1% split\n",
    "        self.train_df, self.val_df, self.test_df = \\\n",
    "          np.split(cbow_df, [int(.98*len(cbow_df)), int(.99*len(cbow_df))])\n",
    "\n",
    "        self._lookup_dict = {'train': self.train_df,\n",
    "                             'val': self.val_df,\n",
    "                             'test': self.test_df}\n",
    "\n",
    "        self.set_split()\n",
    "        self._vectorizer = Vectorizer.from_dataframe(self.train_df)\n",
    "\n",
    "    @classmethod\n",
    "    def load_and_create_dataset(cls, filepath, context_size, frac=1.0):\n",
    "        \"\"\"Load and preprocess the dataset\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): location of the dataset\n",
    "            context_size (int): size of the context before/after the target word\n",
    "            frac (float, optional): fraction of the data to use (default 1.0)\n",
    "        Returns:\n",
    "            an instance of ShakespeareDataset\n",
    "        \"\"\"\n",
    "        # load the file\n",
    "        lines = ShakespeareDataset._load_file(filepath)\n",
    "        # consider the fraction param and throw away the rest\n",
    "        lines = lines[:int(len(lines)*frac)]\n",
    "        \n",
    "        # Preprocess\n",
    "        tokens = ShakespeareDataset._preprocess_and_split_lines(lines)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        dataframe_data = ShakespeareDataset._create_context_data(\n",
    "            tokens, \n",
    "            context_size\n",
    "        )\n",
    "        cbow_df = pd.DataFrame(dataframe_data, columns=['context', 'target'])\n",
    "        \n",
    "        # Create an instance \n",
    "        return cls(cbow_df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_file(filepath):\n",
    "        \"\"\"Load the dataset file into lines\"\"\"\n",
    "        with open(filepath) as file:\n",
    "            lines = file.readlines()\n",
    "            file.close()\n",
    "            return lines\n",
    "    \n",
    "    @staticmethod\n",
    "    def _preprocess_and_split_lines(lines):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            lines (list): a list of lines of the dataset\n",
    "        Returns:\n",
    "            a list of tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        # Regex\n",
    "        lines = lines[134:164924] #these numbers are only valid for the full corpus\n",
    "        text = ''.join(lines)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        #tokens = text.split()\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_context_data(tokens, context_size):\n",
    "        data = []\n",
    "        for i in range(context_size, len(tokens) - context_size):\n",
    "            # Context before w_i\n",
    "            context_before_w = tokens[i - context_size: i]\n",
    "\n",
    "            # Context after w_i\n",
    "            context_after_w = tokens[i + 1: i + context_size + 1]\n",
    "\n",
    "            # Put them together\n",
    "            context_window = context_before_w + context_after_w\n",
    "\n",
    "            # Target = w_i\n",
    "            target = tokens[i]\n",
    "\n",
    "            # Append in the correct format\n",
    "            data.append([context_window, target])\n",
    "        return data\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_df = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = self._vectorizer.vectorize(row.context)\n",
    "        target_index = self._vectorizer.vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aDv7WUgH9Uv"
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlOh92XhH9Uw"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self._context_window_size = context_size * 2\n",
    "        \n",
    "        # Embedding/input layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons) \n",
    "\n",
    "        # Output layer \n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = self.embeddings(inputs).sum(dim=1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds) / self._context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h)\n",
    "         \n",
    "        # output\n",
    "        # also note that we don't compute softmax here because Cross Entropy is used as a loss function\n",
    "        out = F.relu(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7Z8Es9DH9U3"
   },
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPFevBWyP00y"
   },
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.epoch_index = 0\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.model_filename = filename\n",
    "\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"Handle the training state updates.\n",
    "\n",
    "        model (nn.Module): model to save\n",
    "        \"\"\"\n",
    "        # Save one model at least once\n",
    "        if self.epoch_index == 0:\n",
    "            torch.save(model.state_dict(), self.model_filename)\n",
    "\n",
    "        # Save model if performance improved\n",
    "        else:\n",
    "            loss_prev, loss_cur = self.val_loss[-2:]\n",
    "\n",
    "            # compare current loss with the previous one\n",
    "            if loss_cur <= loss_prev:\n",
    "              # save if needed\n",
    "              torch.save(model.state_dict(), self.model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s5TYE7OmH9U9",
    "outputId": "1ac31dd9-7bfa-44cd-bd9a-e5774b22fbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    shakespeare_csv_filepath=\"shakespeare-corpus.txt\",\n",
    "    model_state_file=\"shakespeare_model.pth\",\n",
    "    # Model hyper parameters\n",
    "    context_size=2,\n",
    "    num_neurons=128,\n",
    "    embedding_dim=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=40,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    # Runtime options\n",
    "    cuda=True\n",
    ")\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "class CBOWTrainingRoutine:\n",
    "\n",
    "    def create_new_classifier(self, vocab_len, embedding_dim, context_size,\n",
    "                              nr_hidden_neurons, device, learning_rate,\n",
    "                              filepath):\n",
    "      # Classifier\n",
    "      self.loss_func = nn.CrossEntropyLoss()\n",
    "      classifier = CBOW(\n",
    "          vocab_len, \n",
    "          embedding_dim, \n",
    "          context_size, \n",
    "          nr_hidden_neurons)\n",
    "      self.classifier = classifier.to(device)\n",
    "      self.optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "      filename = str(nr_hidden_neurons) + \"_\" + str(learning_rate) + \"_\" + filepath\n",
    "      self.train_state = TrainState(filename)\n",
    "\n",
    "\n",
    "    def train(self, dataset, num_epochs, batch_size, device):\n",
    "      for epoch_index in tqdm(range(num_epochs)):\n",
    "          self.train_state.epoch_index = epoch_index\n",
    "\n",
    "          # Iterate over training dataset\n",
    "\n",
    "          # setup: batch generator, set loss to 0, set train mode on\n",
    "\n",
    "          dataset.set_split('train')\n",
    "          batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            device=device)\n",
    "          running_loss = 0.0\n",
    "          self.classifier.train()\n",
    "\n",
    "          for batch_index, batch_dict in enumerate(batch_generator):\n",
    "              # the training routine is these 5 steps:\n",
    "\n",
    "              # --------------------------------------\n",
    "              # step 1. zero the gradients\n",
    "              self.optimizer.zero_grad()\n",
    "\n",
    "              # step 2. compute the output\n",
    "              y_pred = self.classifier(batch_dict['x_data'])\n",
    "\n",
    "              # step 3. compute the loss\n",
    "              loss = self.loss_func(y_pred, batch_dict['y_target'])\n",
    "              loss_t = loss.item()\n",
    "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "              # step 4. use loss to produce gradients\n",
    "              loss.backward()\n",
    "\n",
    "              # step 5. use optimizer to take gradient step\n",
    "              self.optimizer.step()\n",
    "              # -----------------------------------------\n",
    "\n",
    "          self.train_state.train_loss.append(running_loss)\n",
    "\n",
    "          # Iterate over val dataset\n",
    "\n",
    "          # setup: batch generator, set loss to 0; set eval mode on\n",
    "          dataset.set_split('val')\n",
    "          batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            device=device)\n",
    "          running_loss = 0.0\n",
    "          self.classifier.eval()\n",
    "\n",
    "          for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "              # compute the output\n",
    "              y_pred =  self.classifier(batch_dict['x_data'])\n",
    "\n",
    "              # compute the loss\n",
    "              loss = self.loss_func(y_pred, batch_dict['y_target'])\n",
    "              loss_t = loss.item()\n",
    "              running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "          self.train_state.val_loss.append(running_loss)\n",
    "\n",
    "          self.train_state.update(model=self.classifier)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhcKzv2fGK0l"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = ShakespeareDataset.load_and_create_dataset(\n",
    "    args.shakespeare_csv_filepath,\n",
    "    args.context_size,\n",
    "    0.01\n",
    ")\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "training_routine = CBOWTrainingRoutine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "4_JsVzIt7F2A",
    "outputId": "a00c2b1f-7195-40c2-ad4f-5aa555471af9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:53,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:06<01:54,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:09<01:49,  2.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:11<01:43,  2.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:14<01:41,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:17<01:38,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:20<01:33,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:23<01:33,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:26<01:33,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:29<01:30,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:32<01:24,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:35<01:23,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:38<01:21,  3.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:41<01:19,  3.05s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:44<01:16,  3.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:47<01:13,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:50<01:10,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [00:54<01:07,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [00:56<01:03,  3.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [00:59<00:59,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:02<00:55,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:06<00:55,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:08<00:50,  2.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:11<00:46,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:14<00:42,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:16<00:39,  2.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:19<00:36,  2.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:22<00:34,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:25<00:31,  2.82s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:28<00:28,  2.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:31<00:25,  2.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:34<00:23,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:37<00:20,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:40<00:18,  3.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:43<00:15,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [01:46<00:11,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [01:49<00:08,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [01:52<00:05,  2.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [01:54<00:02,  2.84s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [01:57<00:00,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 15 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:50,  2.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:05<01:47,  2.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:08<01:45,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:11<01:43,  2.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:14<01:41,  2.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:17<01:38,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:20<01:36,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:23<01:33,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:26<01:31,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:29<01:28,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:32<01:25,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:35<01:22,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:38<01:19,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:41<01:17,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:44<01:14,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:46<01:11,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:49<01:08,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [00:52<01:05,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [00:55<01:02,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [00:58<00:58,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:01<00:55,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:04<00:53,  2.98s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:08<00:53,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:11<00:49,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:14<00:45,  3.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:17<00:42,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:20<00:39,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:23<00:36,  3.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:26<00:34,  3.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:30<00:32,  3.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:33<00:30,  3.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:36<00:25,  3.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:39<00:21,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:42<00:18,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:45<00:15,  3.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [01:48<00:12,  3.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [01:51<00:09,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [01:54<00:05,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [01:57<00:02,  2.98s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:00<00:00,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 30 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:51,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:05<01:48,  2.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:08<01:47,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:11<01:47,  2.98s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:15<01:52,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:19<01:54,  3.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:22<01:52,  3.41s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:26<01:49,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:30<01:52,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:34<01:48,  3.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:37<01:41,  3.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:40<01:36,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:43<01:29,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:46<01:25,  3.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:49<01:21,  3.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:53<01:17,  3.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:56<01:16,  3.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [01:00<01:14,  3.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [01:03<01:08,  3.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:06<01:04,  3.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:09<01:00,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:12<00:57,  3.19s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:15<00:54,  3.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:19<00:50,  3.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:22<00:47,  3.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:25<00:44,  3.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:28<00:41,  3.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:31<00:37,  3.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:34<00:34,  3.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:37<00:31,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:40<00:28,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:44<00:24,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:47<00:21,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:50<00:18,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:53<00:15,  3.14s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [01:56<00:12,  3.14s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [01:59<00:09,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [02:02<00:06,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:05<00:03,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:09<00:00,  3.23s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 50 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:53,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:05<01:51,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:09<01:53,  3.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:12<01:54,  3.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:16<01:54,  3.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:19<01:52,  3.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:22<01:49,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:26<01:47,  3.35s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:29<01:44,  3.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:33<01:41,  3.39s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:37<01:42,  3.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:40<01:41,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:44<01:38,  3.66s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:48<01:36,  3.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:51<01:30,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:55<01:27,  3.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:59<01:24,  3.67s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [01:03<01:21,  3.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [01:06<01:16,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:10<01:16,  3.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:14<01:12,  3.81s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:18<01:09,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:23<01:13,  4.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:29<01:16,  4.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:34<01:09,  4.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:37<01:01,  4.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:41<00:54,  4.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:45<00:47,  3.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:48<00:40,  3.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:51<00:35,  3.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:54<00:31,  3.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:57<00:27,  3.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [02:01<00:23,  3.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [02:04<00:20,  3.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [02:08<00:17,  3.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [02:11<00:13,  3.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [02:15<00:10,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [02:18<00:07,  3.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:22<00:03,  3.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:25<00:00,  3.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:03<02:06,  3.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:06<02:04,  3.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:10<02:11,  3.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:14<02:13,  3.71s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:18<02:13,  3.82s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:22<02:12,  3.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:26<02:09,  3.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:31<02:06,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:35<02:11,  4.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:41<02:15,  4.50s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:45<02:12,  4.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:50<02:07,  4.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:54<01:58,  4.41s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:58<01:51,  4.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [01:02<01:46,  4.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [01:06<01:43,  4.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [01:11<01:38,  4.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [01:15<01:35,  4.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [01:20<01:31,  4.35s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:24<01:26,  4.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:28<01:22,  4.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:32<01:17,  4.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:36<01:11,  4.20s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:40<01:06,  4.19s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:45<01:02,  4.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:49<01:01,  4.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:54<00:56,  4.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:58<00:51,  4.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [02:03<00:48,  4.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [02:09<00:49,  4.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [02:13<00:43,  4.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [02:18<00:38,  4.82s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [02:23<00:33,  4.74s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [02:27<00:26,  4.49s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [02:30<00:21,  4.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [02:34<00:16,  4.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [02:38<00:12,  4.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [02:42<00:08,  4.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:46<00:04,  4.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:50<00:00,  4.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:42,  2.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:06<01:48,  2.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:09<01:47,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:12<01:54,  3.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:16<01:59,  3.42s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:20<01:54,  3.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:23<01:47,  3.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:26<01:42,  3.19s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:29<01:42,  3.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:32<01:36,  3.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:35<01:31,  3.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:40<01:37,  3.49s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:42<01:29,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:46<01:28,  3.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:49<01:20,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:52<01:19,  3.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:56<01:17,  3.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [00:59<01:11,  3.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [01:02<01:06,  3.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:05<01:01,  3.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:07<00:56,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:10<00:52,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:13<00:48,  2.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:16<00:46,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:19<00:44,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:22<00:42,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:25<00:39,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:28<00:36,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:31<00:32,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:34<00:30,  3.04s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:38<00:27,  3.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:41<00:24,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:44<00:21,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:47<00:18,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:50<00:15,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [01:54<00:12,  3.22s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [01:57<00:09,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [02:00<00:06,  3.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:04<00:03,  3.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:07<00:00,  3.19s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 15 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:53,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:05<01:50,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:08<01:49,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:11<01:47,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:14<01:45,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:17<01:42,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:21<01:39,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:24<01:36,  3.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:27<01:33,  3.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:30<01:30,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:33<01:27,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:36<01:24,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:39<01:20,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:41<01:16,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:44<01:13,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:47<01:10,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:50<01:07,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [00:53<01:04,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [00:56<01:01,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [00:59<00:58,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:02<00:55,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:05<00:53,  2.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:08<00:50,  2.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:11<00:47,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:14<00:44,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:17<00:41,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:20<00:38,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:23<00:35,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:25<00:32,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:28<00:29,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:31<00:26,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:34<00:23,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:37<00:20,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:40<00:17,  2.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:43<00:14,  2.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [01:46<00:11,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [01:49<00:08,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [01:52<00:05,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [01:55<00:02,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [01:57<00:00,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 30 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:51,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:05<01:48,  2.85s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:08<01:48,  2.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:11<01:47,  2.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:15<01:45,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:18<01:43,  3.05s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:21<01:41,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:24<01:38,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:27<01:35,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:30<01:32,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:33<01:29,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:36<01:26,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:39<01:23,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:42<01:20,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:45<01:17,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:49<01:13,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:52<01:10,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [00:55<01:07,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [00:58<01:04,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:01<01:01,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:04<00:58,  3.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:07<00:55,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:10<00:52,  3.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:13<00:49,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:16<00:46,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:20<00:43,  3.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:23<00:40,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:26<00:37,  3.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:29<00:34,  3.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:32<00:31,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:35<00:28,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:38<00:24,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:41<00:21,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:44<00:18,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:48<00:15,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [01:51<00:12,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [01:54<00:09,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [01:57<00:06,  3.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:00<00:03,  3.14s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:03<00:00,  3.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 50 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:02<01:53,  2.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:05<01:50,  2.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:09<01:53,  3.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:12<01:53,  3.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:15<01:52,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:19<01:50,  3.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:22<01:48,  3.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:25<01:45,  3.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:29<01:41,  3.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:32<01:38,  3.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:36<01:37,  3.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:39<01:38,  3.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:43<01:38,  3.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:47<01:36,  3.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:51<01:30,  3.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:54<01:24,  3.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [00:57<01:19,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [01:01<01:15,  3.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [01:04<01:10,  3.38s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:07<01:06,  3.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:10<01:03,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:14<00:59,  3.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:17<00:56,  3.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:20<00:53,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:24<00:50,  3.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:27<00:46,  3.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:30<00:43,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:34<00:39,  3.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:37<00:36,  3.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:40<00:33,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:44<00:29,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [01:47<00:26,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [01:50<00:23,  3.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [01:54<00:19,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [01:57<00:16,  3.35s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [02:00<00:13,  3.35s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [02:04<00:10,  3.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [02:07<00:06,  3.41s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:11<00:03,  3.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:14<00:00,  3.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:03<01:57,  3.00s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 2/40 [00:06<01:55,  3.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 3/40 [00:10<02:02,  3.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 4/40 [00:13<02:04,  3.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 5/40 [00:17<02:04,  3.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 6/40 [00:21<02:03,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 7/40 [00:25<02:01,  3.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 8/40 [00:29<01:59,  3.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▎       | 9/40 [00:32<01:56,  3.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 10/40 [00:36<01:53,  3.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 11/40 [00:40<01:49,  3.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 12/40 [00:44<01:46,  3.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▎      | 13/40 [00:48<01:42,  3.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 14/40 [00:52<01:39,  3.81s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 15/40 [00:55<01:35,  3.81s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 16/40 [00:59<01:31,  3.82s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▎     | 17/40 [01:03<01:28,  3.84s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 18/40 [01:07<01:25,  3.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 19/40 [01:11<01:21,  3.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 20/40 [01:15<01:17,  3.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▎    | 21/40 [01:19<01:13,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 22/40 [01:22<01:09,  3.88s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▊    | 23/40 [01:26<01:06,  3.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 24/40 [01:30<01:02,  3.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 25/40 [01:34<00:58,  3.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 26/40 [01:38<00:54,  3.89s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 27/40 [01:42<00:50,  3.90s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 28/40 [01:46<00:46,  3.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▎  | 29/40 [01:50<00:43,  3.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 30/40 [01:54<00:39,  3.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 31/40 [01:58<00:35,  3.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 32/40 [02:02<00:31,  3.93s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▎ | 33/40 [02:06<00:27,  3.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 34/40 [02:10<00:23,  3.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 35/40 [02:14<00:19,  3.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 36/40 [02:17<00:15,  3.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▎| 37/40 [02:21<00:11,  3.95s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 38/40 [02:25<00:07,  3.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████▊| 39/40 [02:29<00:03,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 40/40 [02:33<00:00,  3.85s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "grid_search_params = {\n",
    "      \"lr\": [0.001, 0.01],\n",
    "      \"nr_hidden_neurons\": [2, 15, 30, 50, 100]\n",
    "}\n",
    "\n",
    "\n",
    "values = [lists for _, lists in grid_search_params.items()]\n",
    "classifiers = []\n",
    "\n",
    "for lr, nr_hidden in itertools.product(*values):\n",
    "    print(\"Training \" + str(nr_hidden) + \" \" + str(lr))\n",
    "    training_routine.create_new_classifier(\n",
    "      len(vectorizer.vocab), args.embedding_dim, \n",
    "      args.context_size, nr_hidden, \n",
    "      args.device, lr, args.model_state_file\n",
    "    )\n",
    "\n",
    "    training_routine.train(\n",
    "      dataset, \n",
    "      args.num_epochs,\n",
    "      args.batch_size,\n",
    "      args.device,\n",
    "    )\n",
    "\n",
    "    classifiers.append((training_routine.classifier, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjhH-ovXH9VA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8Mq0Mf5H9VC"
   },
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of pretrained models (classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (linear2): Linear(in_features=50, out_features=2129, bias=True)\n",
       "  ), 0.01), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=2129, bias=True)\n",
       "  ), 0.01), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=15, bias=True)\n",
       "    (linear2): Linear(in_features=15, out_features=2129, bias=True)\n",
       "  ), 0.001), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=2129, bias=True)\n",
       "  ), 0.001), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (linear2): Linear(in_features=30, out_features=2129, bias=True)\n",
       "  ), 0.001), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
       "    (linear2): Linear(in_features=2, out_features=2129, bias=True)\n",
       "  ), 0.001), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=15, bias=True)\n",
       "    (linear2): Linear(in_features=15, out_features=2129, bias=True)\n",
       "  ), 0.01), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
       "    (linear2): Linear(in_features=2, out_features=2129, bias=True)\n",
       "  ), 0.01), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (linear2): Linear(in_features=50, out_features=2129, bias=True)\n",
       "  ), 0.001), (CBOW(\n",
       "    (embeddings): Embedding(2129, 50)\n",
       "    (linear1): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (linear2): Linear(in_features=30, out_features=2129, bias=True)\n",
       "  ), 0.01)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "classifiers_loaded = []\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".pth\"):\n",
    "        # get the number of neurons from filename\n",
    "        str_hidden, str_lr, *rest = file.split(\"_\")\n",
    "        # init the classifier\n",
    "        classifier = CBOW(len(vectorizer.vocab), args.embedding_dim, args.context_size, int(str_hidden))\n",
    "        # load the weights / embeddings\n",
    "        classifier.load_state_dict(torch.load(file))\n",
    "        # set to eval mode\n",
    "        classifier.eval()\n",
    "        # add to the list of loaded classifiers\n",
    "        classifiers_loaded.append((classifier, float(str_lr)))\n",
    "        \n",
    "classifiers_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5Va332XH9VC"
   },
   "outputs": [],
   "source": [
    "def get_closest_word_pwd(classifier, word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = classifier.embeddings\n",
    "    test_vocab = dataset.get_vectorizer().vocab\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = test_vocab.lookup_token(word)\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long).to(args.device)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long).to(args.device)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.lookup_index(j), float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]\n",
    "\n",
    "def get_closest_word_cs(classifier, word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = classifier.embeddings\n",
    "    test_vocab = dataset.get_vectorizer().vocab\n",
    "    pdist = nn.CosineSimilarity()\n",
    "    i = test_vocab.lookup_token(word)\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long).to(args.device)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long).to(args.device)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.lookup_index(j), float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[::-1][:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Classifier (LR: 0.01): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[9.79] - sweet-seasoned\n",
      "...[10.35] - stealing\n",
      "...[10.35] - measured\n",
      "...[10.54] - himself\n",
      "...[10.59] - tires\n",
      "...[0.55] - measured\n",
      "...[0.51] - sweet-seasoned\n",
      "...[0.51] - pearl\n",
      "...[0.48] - sins\n",
      "...[0.47] - quicker\n",
      "==================================================\n",
      "Classifier (LR: 0.01): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[8.78] - delight\n",
      "...[9.04] - ’\n",
      "...[9.06] - bell\n",
      "...[9.22] - quick\n",
      "...[9.30] - faith\n",
      "...[0.58] - delight\n",
      "...[0.54] - imprint\n",
      "...[0.46] - lover\n",
      "...[0.45] - ’\n",
      "...[0.45] - quick\n",
      "==================================================\n",
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=15, bias=True)\n",
      "  (linear2): Linear(in_features=15, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[7.63] - found\n",
      "...[7.95] - pour\n",
      "...[8.20] - brow\n",
      "...[8.23] - vanished\n",
      "...[8.25] - him\n",
      "...[0.48] - found\n",
      "...[0.44] - twixt\n",
      "...[0.43] - persuade\n",
      "...[0.42] - strange\n",
      "...[0.42] - glad\n",
      "==================================================\n",
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[8.32] - want\n",
      "...[8.37] - well-contented\n",
      "...[8.41] - benefit\n",
      "...[8.57] - please\n",
      "...[8.67] - enforced\n",
      "...[0.44] - well-contented\n",
      "...[0.43] - benefit\n",
      "...[0.40] - enforced\n",
      "...[0.40] - character\n",
      "...[0.38] - wind\n",
      "==================================================\n",
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=30, bias=True)\n",
      "  (linear2): Linear(in_features=30, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[6.98] - blessed\n",
      "...[7.17] - never\n",
      "...[7.35] - fresh\n",
      "...[7.47] - sorrow\n",
      "...[7.49] - thee\n",
      "...[0.49] - blessed\n",
      "...[0.49] - fresh\n",
      "...[0.43] - never\n",
      "...[0.43] - report\n",
      "...[0.41] - impiety\n",
      "==================================================\n",
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[7.73] - windows\n",
      "...[7.76] - weakens\n",
      "...[7.77] - lend\n",
      "...[7.86] - lodged\n",
      "...[7.93] - tires\n",
      "...[0.51] - windows\n",
      "...[0.48] - leads\n",
      "...[0.45] - teeth\n",
      "...[0.44] - lodged\n",
      "...[0.43] - early\n",
      "==================================================\n",
      "Classifier (LR: 0.01): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=15, bias=True)\n",
      "  (linear2): Linear(in_features=15, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[10.48] - beauty\n",
      "...[10.60] - sparkling\n",
      "...[10.66] - curse\n",
      "...[10.79] - towers\n",
      "...[10.85] - poet\n",
      "...[0.64] - poet\n",
      "...[0.58] - pays\n",
      "...[0.55] - flame\n",
      "...[0.53] - curse\n",
      "...[0.51] - unseeing\n",
      "==================================================\n",
      "Classifier (LR: 0.01): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[8.06] - seeing\n",
      "...[8.14] - not\n",
      "...[8.19] - showers\n",
      "...[8.20] - cast\n",
      "...[8.35] - metre\n",
      "...[0.65] - written\n",
      "...[0.62] - jump\n",
      "...[0.61] - restful\n",
      "...[0.60] - seasons\n",
      "...[0.59] - crushed\n",
      "==================================================\n",
      "Classifier (LR: 0.001): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[7.91] - graves\n",
      "...[7.92] - injurious\n",
      "...[7.95] - ne\n",
      "...[8.02] - solemn\n",
      "...[8.07] - toiled\n",
      "...[0.46] - livery\n",
      "...[0.46] - although\n",
      "...[0.45] - toiled\n",
      "...[0.43] - graves\n",
      "...[0.43] - solemn\n",
      "==================================================\n",
      "Classifier (LR: 0.01): CBOW(\n",
      "  (embeddings): Embedding(2129, 50)\n",
      "  (linear1): Linear(in_features=50, out_features=30, bias=True)\n",
      "  (linear2): Linear(in_features=30, out_features=2129, bias=True)\n",
      ")\n",
      "\n",
      "...[8.44] - win\n",
      "...[8.68] - that\n",
      "...[8.78] - grows\n",
      "...[8.92] - desire\n",
      "...[8.97] - can\n",
      "...[0.45] - spoil\n",
      "...[0.42] - part\n",
      "...[0.42] - foison\n",
      "...[0.40] - fear\n",
      "...[0.38] - well-tuned\n"
     ]
    }
   ],
   "source": [
    "target_classifiers = classifiers if not classifiers else classifiers_loaded\n",
    "\n",
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    Pretty print embedding results.\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "for classifier, lr in target_classifiers:\n",
    "    word = 'king'\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Classifier (LR: \" + str(lr) + \"): \" + str(classifier) + \"\\n\")\n",
    "    print(\"===Pairwise Distance===\")\n",
    "    pretty_print(get_closest_word_pwd(classifier, word))\n",
    "    print(\"===Cosine Similarit\")\n",
    "    pretty_print(get_closest_word_cs(classifier, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ga2LBQWH9VE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_examples = len(data)\n",
    "pred_sum = 0 # softmax check\n",
    "acc_sum = 0 # accuracy\n",
    "\n",
    "for i in range(nr_examples):\n",
    "    ids = vectorizer.vectorize(data[i][0])\n",
    "    target = test_vocab.tok_to_ids[data[i][1]]\n",
    "    pred = model(ids) # prediction\n",
    "    pred_sum += pred.squeeze().sum().item() \n",
    "    \n",
    "    _, pred_index = pred.max(dim=1) # prediction index\n",
    "    n_correct = torch.eq(pred_index, target)\n",
    "    acc_sum += n_correct.item()\n",
    "    \n",
    "    print(\"Prediction: \" + str(pred_index.item()), \"| Target: \" + str(target))\n",
    "    \n",
    "print(acc_sum / nr_examples)\n",
    "print(pred_sum / nr_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzHWb514H9VK"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J3uP5ASH9VK"
   },
   "outputs": [],
   "source": [
    "stringo = \"here is an [_exit_]\"\n",
    "stringo = re.sub('(\\[_).*(_\\])', '', stringo)\n",
    "print(stringo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0Hx59HFH9VN"
   },
   "outputs": [],
   "source": [
    "#finis is 164924\n",
    "#beginngin is line 134 --> just keep what's in between those lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tM0tVkRH9VQ"
   },
   "outputs": [],
   "source": [
    "filename = 'shakespeare-corpus.txt'\n",
    "file = open(filename)\n",
    "lines = file.readlines()\n",
    "lines = lines[134:164924]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmo_YhzQH9VR"
   },
   "outputs": [],
   "source": [
    "def mytext(lines):\n",
    "    corpus = ''\n",
    "    for line in lines:\n",
    "        text = re.sub(r'\\d+', '', line)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        corpus += text\n",
    "    return corpus\n",
    "\n",
    "%time len(mytext(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QB0kV7wJH9VS"
   },
   "outputs": [],
   "source": [
    "def mytext2(lines):\n",
    "    text = ''.join(lines)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('SCENE \\S', '', text)\n",
    "    text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "    text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "%time len(mytext2(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGJthXarH9VS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJdHFnv9JQfm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cbow_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
