{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DQTQQGF_TMcD",
    "outputId": "860d0c42-0384-4202-ef8b-b8370ef7d46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11cea2470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, filepath):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.tokens = self.nltk_tokenize()\n",
    "        self.tok_to_ids, self.ids_to_tok = self.make_dicts()\n",
    "    \n",
    "    def readfile(self):\n",
    "        \"\"\"this function opens the file and returns the text in a string\"\"\"\n",
    "        file = open(self.filepath)\n",
    "        lines = file.readlines()\n",
    "        #lines = lines[134:164924] #these numbers are only valid for the full corpus\n",
    "        text = ''.join(lines)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        file.close()\n",
    "        return text\n",
    "    \n",
    "    def nltk_tokenize(self):\n",
    "        \"\"\"this function tokenizes the text and returns a list of tokens as strings\"\"\"\n",
    "        text = self.readfile()\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = list(set(self.tokens))\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tok_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.tok_to_ids[w] for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (Vectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "        \n",
    "        self.train_df = self.cbow_df[self.cbow_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split=='train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'test_corpus.txt'\n",
    "test_vocab = Vocabulary(filepath)\n",
    "vectorizer = Vectorizer(test_vocab)\n",
    "\n",
    "# Size of the context windows, 2 and 5 are supposed to be used in ex02...\n",
    "# range \\in [2, 1/2 * document_length - 1]\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# let's stick with this notation for now ;)\n",
    "CONTEXT_WINDOW_SIZE = CONTEXT_SIZE * 2\n",
    "\n",
    "\n",
    "# Data creation - get context around the target word\n",
    "data = []\n",
    "tokens = test_vocab.tokens\n",
    "for i in range(CONTEXT_SIZE, len(tokens) - CONTEXT_SIZE):\n",
    "    # Context before w_i\n",
    "    context_before_w = tokens[i - CONTEXT_SIZE: i]\n",
    "    \n",
    "    # Context after w_i\n",
    "    context_after_w = tokens[i + 1: i + CONTEXT_SIZE + 1]\n",
    "    \n",
    "    # Put them together\n",
    "    context_window = context_before_w + context_after_w\n",
    "    \n",
    "    # Target = w_i\n",
    "    target = tokens[i]\n",
    "    \n",
    "    # Append in the correct format\n",
    "    data.append((context_window, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # note: this probably doesn't deal with 'UNK' words\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons)  \n",
    "        \n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = sum(self.embeddings(inputs))\n",
    "\n",
    "        # shape = (1, EMBEDDING_DIM)\n",
    "        # -1 param in view() ... \"the actual value for this dimension will be inferred so that the number of elements in the view matches the original number of elements.\"\n",
    "        embeds_2D = embeds.view(1, -1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds_2D) / self.context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h) \n",
    "         \n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.5256, -0.7502, -0.6540,  ..., -0.6298, -0.9274,  0.5451],\n",
      "        [ 0.0663, -0.4370,  0.7626,  ...,  1.1899,  0.8165, -0.9135],\n",
      "        [ 1.3851, -0.8138, -0.9276,  ...,  0.6419,  0.4730, -0.4286],\n",
      "        ...,\n",
      "        [ 0.2124,  0.9873, -0.2969,  ..., -2.1730,  0.1277, -1.1812],\n",
      "        [ 0.0054, -0.3642,  0.4567,  ..., -1.5041, -0.7924,  0.0683],\n",
      "        [ 1.0057,  0.0652,  1.9921,  ...,  0.4940,  1.0178,  0.2038]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:51<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2237.4248569607735, 1844.4015293334378, 1440.088591142383, 1177.86727492285, 980.0229213272937, 793.3261760807657, 575.778041950732, 433.486690040499, 324.5412144839497, 278.13774361486765, 238.39929716328274, 250.06337988294754, 311.45733652755115, 290.01929304948766, 357.4656196564234, 270.4255312552793, 289.0232770008211, 308.60147824103416, 145.4356349904693, 152.0936584054103, 125.49669334381667, 166.29700606119587, 103.37830632951432, 89.33954691696636, 85.15715585936584, 95.21529411287358, 75.77724789247753, 90.0151629171369, 65.29424973829325, 76.90503726569227, 40.612655067431476, 39.293238996233924, 51.0221944406564, 119.78689336136671, 179.18098528433043, 429.7664403114887, 212.89152821329915, 112.0458070247542, 62.4161452896411, 56.112956064284106, 152.43835431704966, 183.48477792695712, 226.58457896315727, 237.59952252915934, 201.69039237422086, 111.87093706398832, 132.6249386274413, 88.59827452483475, 76.38500837930897, 43.89748523074262, 23.755856104254313, 21.54025441128362, 28.429765259537213, 14.707104036428447, 10.779811926222209, 8.470675296918657, 8.440490958437003, 8.12695514480341, 11.054854064163571, 8.003435844366876, 8.361664630655106, 45.75536083093293, 380.6890796644201, 596.1386189499324, 138.65695910992835, 144.9571204253729, 90.85317486158576, 12.88643375650738, 10.983206049501653, 8.102865554749549, 9.418661012502291, 7.382942368239519, 8.134048341719712, 6.915680123047522, 7.244528210201828, 6.675131062323551, 6.949601565546686, 6.974742846569583, 16.71377647473563, 81.93413320328023, 504.9240984699385, 368.38833562693077, 430.843637039991, 122.14875723787833, 105.77971662912768, 10.325531894660791, 6.351048891844314, 6.625087049783659, 5.843617936561365, 5.948032632340443, 5.56519283529807, 5.522323220173448, 5.369823980853859, 5.251296288253904, 5.236162853622254, 5.050810054687432, 5.170101071994722, 5.350877044508465, 17.333850341725395, 93.39822086188025]\n",
      "Parameter containing:\n",
      "tensor([[-2.9349, -2.4576, -1.4579,  ..., -0.8531, -2.1468,  0.6318],\n",
      "        [-0.7202, -0.8995,  0.6102,  ...,  1.0083, -0.0831, -0.0513],\n",
      "        [ 0.0509, -0.1840, -0.6477,  ...,  0.7255,  1.7792,  0.6331],\n",
      "        ...,\n",
      "        [ 0.3644,  1.7076,  0.2008,  ..., -1.5158,  0.4447, -0.2562],\n",
      "        [ 0.9363, -1.8555, -0.3623,  ..., -0.2591,  0.8512,  0.2035],\n",
      "        [ 1.9113, -0.9155,  2.0031,  ...,  0.5826, -0.4632,  1.1938]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_ITERATIONS = 100\n",
    "NUM_NEURONS = 128\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = CBOW(len(test_vocab), EMBEDDING_DIM, CONTEXT_WINDOW_SIZE, NUM_NEURONS)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model.embeddings.weight)\n",
    "\n",
    "for epoch in tqdm(range(NUM_ITERATIONS)):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        # Step1. Create input vector \n",
    "        context_vector_ids = vectorizer.vectorize(context)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        softmax = model(context_vector_ids)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        target = torch.tensor(vectorizer.vocab.tok_to_ids[target], dtype=torch.long).view(1)\n",
    "        loss = loss_function(softmax, target)\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print(losses)\n",
    "print(model.embeddings.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## OOP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shakespeare_csv_filepath = 'test_corpus.txt'\n",
    "#dataset = ShakespeareDataset.load_dataset_and_make_vectorizer(shakespeare_csv_filepath)\n",
    "#dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "#vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "#classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab), embedding_size=args.embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fairest', 9.58243179321289),\n",
       " ('thy', 10.376667976379395),\n",
       " ('s', 10.61782169342041),\n",
       " ('from', 10.686042785644531),\n",
       " ('answer', 10.723249435424805)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part2 supplied function\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = model.embeddings\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = test_vocab.tok_to_ids[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long) \n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.ids_to_tok[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1]) \n",
    "    return word_distance[:topn]\n",
    "\n",
    "get_closest_word('desire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7    | Target: 7\n",
      "Prediction: 145    | Target: 145\n",
      "Prediction: 150    | Target: 150\n",
      "Prediction: 140    | Target: 140\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 133    | Target: 133\n",
      "Prediction: 43    | Target: 43\n",
      "Prediction: 21    | Target: 21\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 91    | Target: 91\n",
      "Prediction: 85    | Target: 85\n",
      "Prediction: 55    | Target: 55\n",
      "Prediction: 170    | Target: 170\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 90    | Target: 90\n",
      "Prediction: 12    | Target: 12\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 202    | Target: 202\n",
      "Prediction: 200    | Target: 200\n",
      "Prediction: 120    | Target: 120\n",
      "Prediction: 18    | Target: 18\n",
      "Prediction: 0    | Target: 0\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 15    | Target: 15\n",
      "Prediction: 80    | Target: 80\n",
      "Prediction: 176    | Target: 176\n",
      "Prediction: 85    | Target: 85\n",
      "Prediction: 6    | Target: 6\n",
      "Prediction: 15    | Target: 15\n",
      "Prediction: 26    | Target: 26\n",
      "Prediction: 144    | Target: 124\n",
      "Prediction: 90    | Target: 90\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 56    | Target: 56\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 11    | Target: 11\n",
      "Prediction: 31    | Target: 31\n",
      "Prediction: 197    | Target: 197\n",
      "Prediction: 113    | Target: 113\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 107    | Target: 107\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 94    | Target: 94\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 28    | Target: 28\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 52    | Target: 52\n",
      "Prediction: 144    | Target: 171\n",
      "Prediction: 122    | Target: 122\n",
      "Prediction: 19    | Target: 19\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 33    | Target: 33\n",
      "Prediction: 163    | Target: 163\n",
      "Prediction: 88    | Target: 88\n",
      "Prediction: 114    | Target: 114\n",
      "Prediction: 99    | Target: 99\n",
      "Prediction: 57    | Target: 57\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 117    | Target: 117\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 179    | Target: 179\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 96    | Target: 96\n",
      "Prediction: 117    | Target: 117\n",
      "Prediction: 39    | Target: 39\n",
      "Prediction: 112    | Target: 112\n",
      "Prediction: 144    | Target: 124\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 133    | Target: 133\n",
      "Prediction: 167    | Target: 167\n",
      "Prediction: 69    | Target: 69\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 159    | Target: 159\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 153    | Target: 153\n",
      "Prediction: 66    | Target: 66\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 77    | Target: 77\n",
      "Prediction: 191    | Target: 191\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 121    | Target: 121\n",
      "Prediction: 79    | Target: 79\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 155    | Target: 155\n",
      "Prediction: 11    | Target: 11\n",
      "Prediction: 31    | Target: 31\n",
      "Prediction: 25    | Target: 25\n",
      "Prediction: 73    | Target: 73\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 53    | Target: 53\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 80    | Target: 80\n",
      "Prediction: 136    | Target: 136\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 164    | Target: 164\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 94    | Target: 94\n",
      "Prediction: 34    | Target: 34\n",
      "Prediction: 45    | Target: 45\n",
      "Prediction: 131    | Target: 131\n",
      "Prediction: 124    | Target: 124\n",
      "Prediction: 50    | Target: 50\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 159    | Target: 159\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 157    | Target: 157\n",
      "Prediction: 74    | Target: 74\n",
      "Prediction: 128    | Target: 128\n",
      "Prediction: 175    | Target: 175\n",
      "Prediction: 135    | Target: 135\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 4    | Target: 4\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 159    | Target: 159\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 177    | Target: 177\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 120    | Target: 120\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 83    | Target: 83\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 146    | Target: 146\n",
      "Prediction: 67    | Target: 67\n",
      "Prediction: 139    | Target: 139\n",
      "Prediction: 147    | Target: 147\n",
      "Prediction: 168    | Target: 168\n",
      "Prediction: 196    | Target: 196\n",
      "Prediction: 198    | Target: 198\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 192    | Target: 192\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 23    | Target: 23\n",
      "Prediction: 13    | Target: 13\n",
      "Prediction: 126    | Target: 126\n",
      "Prediction: 45    | Target: 45\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 21    | Target: 21\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 72    | Target: 59\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 72    | Target: 72\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 149    | Target: 149\n",
      "Prediction: 173    | Target: 173\n",
      "Prediction: 141    | Target: 141\n",
      "Prediction: 24    | Target: 24\n",
      "Prediction: 98    | Target: 98\n",
      "Prediction: 69    | Target: 69\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 30    | Target: 30\n",
      "Prediction: 135    | Target: 135\n",
      "Prediction: 163    | Target: 163\n",
      "Prediction: 116    | Target: 116\n",
      "Prediction: 201    | Target: 201\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 68    | Target: 68\n",
      "Prediction: 142    | Target: 142\n",
      "Prediction: 189    | Target: 189\n",
      "Prediction: 124    | Target: 124\n",
      "Prediction: 82    | Target: 82\n",
      "Prediction: 1    | Target: 1\n",
      "Prediction: 87    | Target: 87\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 114    | Target: 114\n",
      "Prediction: 132    | Target: 132\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 21    | Target: 21\n",
      "Prediction: 132    | Target: 57\n",
      "Prediction: 65    | Target: 144\n",
      "Prediction: 114    | Target: 114\n",
      "Prediction: 132    | Target: 132\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 51    | Target: 51\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 5    | Target: 5\n",
      "Prediction: 148    | Target: 148\n",
      "Prediction: 115    | Target: 115\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 42    | Target: 42\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 155    | Target: 155\n",
      "Prediction: 11    | Target: 11\n",
      "Prediction: 31    | Target: 31\n",
      "Prediction: 13    | Target: 13\n",
      "Prediction: 152    | Target: 152\n",
      "Prediction: 113    | Target: 113\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 60    | Target: 60\n",
      "Prediction: 35    | Target: 35\n",
      "Prediction: 101    | Target: 101\n",
      "Prediction: 108    | Target: 108\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 105    | Target: 105\n",
      "Prediction: 61    | Target: 61\n",
      "Prediction: 67    | Target: 67\n",
      "Prediction: 123    | Target: 123\n",
      "Prediction: 174    | Target: 174\n",
      "Prediction: 89    | Target: 89\n",
      "Prediction: 61    | Target: 61\n",
      "Prediction: 32    | Target: 32\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 151    | Target: 151\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 21    | Target: 21\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 190    | Target: 48\n",
      "Prediction: 20    | Target: 20\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 27    | Target: 27\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 76    | Target: 76\n",
      "Prediction: 75    | Target: 75\n",
      "Prediction: 161    | Target: 161\n",
      "Prediction: 128    | Target: 128\n",
      "Prediction: 44    | Target: 44\n",
      "Prediction: 185    | Target: 185\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 54    | Target: 54\n",
      "Prediction: 196    | Target: 196\n",
      "Prediction: 195    | Target: 195\n",
      "Prediction: 49    | Target: 49\n",
      "Prediction: 109    | Target: 109\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 188    | Target: 188\n",
      "Prediction: 49    | Target: 49\n",
      "Prediction: 8    | Target: 8\n",
      "Prediction: 162    | Target: 162\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 190    | Target: 190\n",
      "Prediction: 15    | Target: 15\n",
      "Prediction: 21    | Target: 21\n",
      "Prediction: 120    | Target: 120\n",
      "Prediction: 81    | Target: 81\n",
      "Prediction: 11    | Target: 11\n",
      "Prediction: 67    | Target: 67\n",
      "Prediction: 128    | Target: 128\n",
      "Prediction: 186    | Target: 60\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 135    | Target: 135\n",
      "Prediction: 186    | Target: 186\n",
      "Prediction: 102    | Target: 102\n",
      "Prediction: 139    | Target: 139\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 37    | Target: 167\n",
      "Prediction: 8    | Target: 8\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 143    | Target: 143\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 86    | Target: 86\n",
      "Prediction: 37    | Target: 37\n",
      "Prediction: 139    | Target: 139\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 78    | Target: 78\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 94    | Target: 94\n",
      "Prediction: 193    | Target: 193\n",
      "Prediction: 64    | Target: 64\n",
      "Prediction: 67    | Target: 67\n",
      "Prediction: 9    | Target: 9\n",
      "Prediction: 45    | Target: 45\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 63    | Target: 63\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 110    | Target: 110\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 130    | Target: 130\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 154    | Target: 154\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 69    | Target: 69\n",
      "Prediction: 29    | Target: 29\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 18    | Target: 18\n",
      "Prediction: 133    | Target: 133\n",
      "Prediction: 130    | Target: 130\n",
      "Prediction: 200    | Target: 200\n",
      "Prediction: 187    | Target: 187\n",
      "Prediction: 84    | Target: 156\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 14    | Target: 14\n",
      "Prediction: 153    | Target: 153\n",
      "Prediction: 178    | Target: 178\n",
      "Prediction: 27    | Target: 27\n",
      "Prediction: 69    | Target: 69\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 129    | Target: 129\n",
      "Prediction: 182    | Target: 182\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 182    | Target: 84\n",
      "Prediction: 38    | Target: 38\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 159    | Target: 159\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 70    | Target: 70\n",
      "Prediction: 181    | Target: 181\n",
      "Prediction: 41    | Target: 41\n",
      "Prediction: 67    | Target: 67\n",
      "Prediction: 3    | Target: 3\n",
      "Prediction: 114    | Target: 114\n",
      "Prediction: 29    | Target: 29\n",
      "Prediction: 3    | Target: 10\n",
      "Prediction: 141    | Target: 141\n",
      "Prediction: 44    | Target: 44\n",
      "Prediction: 14    | Target: 14\n",
      "Prediction: 106    | Target: 106\n",
      "Prediction: 2    | Target: 2\n",
      "Prediction: 16    | Target: 16\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 46    | Target: 46\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 93    | Target: 93\n",
      "Prediction: 58    | Target: 58\n",
      "Prediction: 157    | Target: 157\n",
      "Prediction: 47    | Target: 47\n",
      "Prediction: 29    | Target: 29\n",
      "Prediction: 194    | Target: 194\n",
      "Prediction: 141    | Target: 141\n",
      "Prediction: 199    | Target: 199\n",
      "Prediction: 30    | Target: 30\n",
      "Prediction: 135    | Target: 135\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 125    | Target: 125\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 15    | Target: 15\n",
      "Prediction: 17    | Target: 17\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 92    | Target: 92\n",
      "Prediction: 10    | Target: 62\n",
      "Prediction: 58    | Target: 58\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 167    | Target: 167\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 41    | Target: 41\n",
      "Prediction: 36    | Target: 36\n",
      "Prediction: 48    | Target: 48\n",
      "Prediction: 63    | Target: 63\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 10    | Target: 10\n",
      "Prediction: 45    | Target: 45\n",
      "Prediction: 146    | Target: 146\n",
      "Prediction: 103    | Target: 103\n",
      "Prediction: 166    | Target: 166\n",
      "Prediction: 134    | Target: 134\n",
      "Prediction: 71    | Target: 71\n",
      "Prediction: 183    | Target: 183\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 137    | Target: 137\n",
      "Prediction: 172    | Target: 172\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 141    | Target: 141\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 160    | Target: 160\n",
      "Prediction: 169    | Target: 169\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 11    | Target: 11\n",
      "Prediction: 138    | Target: 138\n",
      "Prediction: 95    | Target: 95\n",
      "Prediction: 143    | Target: 143\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 118    | Target: 118\n",
      "Prediction: 22    | Target: 22\n",
      "Prediction: 165    | Target: 165\n",
      "Prediction: 128    | Target: 128\n",
      "Prediction: 65    | Target: 65\n",
      "Prediction: 111    | Target: 111\n",
      "Prediction: 18    | Target: 18\n",
      "Prediction: 67    | Target: 67\n",
      "Prediction: 90    | Target: 90\n",
      "Prediction: 27    | Target: 27\n",
      "Prediction: 180    | Target: 180\n",
      "Prediction: 119    | Target: 119\n",
      "Prediction: 104    | Target: 104\n",
      "Prediction: 129    | Target: 129\n",
      "Prediction: 158    | Target: 158\n",
      "Prediction: 135    | Target: 135\n",
      "Prediction: 144    | Target: 144\n",
      "Prediction: 170    | Target: 170\n",
      "Prediction: 97    | Target: 97\n",
      "Prediction: 40    | Target: 40\n",
      "Prediction: 11    | Target: 11\n",
      "Prediction: 127    | Target: 127\n",
      "Prediction: 100    | Target: 100\n",
      "Prediction: 171    | Target: 171\n",
      "0.9679802955665024\n",
      "-49536.01115394931\n"
     ]
    }
   ],
   "source": [
    "nr_examples = len(data)\n",
    "pred_sum = 0 # softmax check\n",
    "acc_sum = 0 # accuracy\n",
    "\n",
    "for i in range(nr_examples):\n",
    "    ids = vectorizer.vectorize(data[i][0])\n",
    "    target = test_vocab.tok_to_ids[data[i][1]]\n",
    "    pred = model(ids) # prediction\n",
    "    pred_sum += pred.squeeze().sum().item() \n",
    "    \n",
    "    _, pred_index = pred.max(dim=1) # prediction index\n",
    "    n_correct = torch.eq(pred_index, target)\n",
    "    acc_sum += n_correct.item()\n",
    "    \n",
    "    print(\"Prediction: \" + str(pred_index.item()), \"| Target: \" + str(target))\n",
    "    \n",
    "print(acc_sum / nr_examples)\n",
    "print(pred_sum / nr_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is an \n"
     ]
    }
   ],
   "source": [
    "stringo = \"here is an [_exit_]\"\n",
    "stringo = re.sub('(\\[_).*(_\\])', '', stringo)\n",
    "print(stringo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finis is 164924\n",
    "#beginngin is line 134 --> just keep what's in between those lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'shakespeare-corpus.txt'\n",
    "file = open(filename)\n",
    "lines = file.readlines()\n",
    "lines = lines[134:164924]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 s, sys: 7.8 ms, total: 1.1 s\n",
      "Wall time: 991 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5521081"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mytext(lines):\n",
    "    corpus = ''\n",
    "    for line in lines:\n",
    "        text = re.sub(r'\\d+', '', line)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        corpus += text\n",
    "    return corpus\n",
    "\n",
    "%time len(mytext(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 291 ms, sys: 24.7 ms, total: 315 ms\n",
      "Wall time: 314 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5521081"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mytext2(lines):\n",
    "    text = ''.join(lines)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('SCENE \\S', '', text)\n",
    "    text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "    text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "%time len(mytext2(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of word_embeddings_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
