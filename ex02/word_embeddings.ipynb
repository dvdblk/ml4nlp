{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DQTQQGF_TMcD",
    "outputId": "860d0c42-0384-4202-ef8b-b8370ef7d46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11333b470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, filepath):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.tokens = self.nltk_tokenize()\n",
    "        self.tok_to_ids, self.ids_to_tok = self.make_dicts()\n",
    "        self.nr_unique_tokens = len(self.vocabulary_set())\n",
    "    \n",
    "    def readfile(self):\n",
    "        \"\"\"this function opens the file and returns the text in a string\"\"\"\n",
    "        file = open(self.filepath)\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "    \n",
    "    def nltk_tokenize(self):\n",
    "        \"\"\"this function tokenizes the text and returns a list of tokens as strings\"\"\"\n",
    "        text = self.readfile()\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = self.vocabulary_set()\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = Vocabulary('shakespeare-corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Hot_Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        self.tok_to_ids = self.vocab.tok_to_ids\n",
    "        self.size = self.vocab.size\n",
    "        self.vectors = self.vectorize()\n",
    "        \n",
    "    def vectorize(self):\n",
    "        dict_vect = {}\n",
    "        for token in self.tok_to_ids:\n",
    "            vector = np.zeros(self.size)\n",
    "            tok_id = self.tok_to_ids[token]\n",
    "            vector[tok_id] = 1\n",
    "            dict_vect.update({token: vector})\n",
    "        return dict_vect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        self.vectors = self.vectorize()\n",
    "        \n",
    "    def vectorize(self):\n",
    "        vectors = {}\n",
    "        for word in self.vocab.tok_to_ids:\n",
    "            vec = torch.rand(50, requires_grad=True) \n",
    "            vectors.update({word: vec}) #each word points to its vector\n",
    "        return vectors\n",
    "    \n",
    "    def make_context_vector(self, context):\n",
    "        ids = [self.vectors[word] for word in context]\n",
    "        return torch.stack(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = 'test_corpus.txt'\n",
    "test_vocab = Vocabulary(mytext)\n",
    "vectorizer = Vectorizer(test_vocab)\n",
    "\n",
    "# Size of the context windows, 2 and 5 are supposed to be used in ex02...\n",
    "# range \\in [2, 1/2 * document_length - 1]\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# let's stick with this notation for now ;)\n",
    "CONTEXT_WINDOW_SIZE = CONTEXT_SIZE * 2\n",
    "\n",
    "NUM_ITERATIONS = 100\n",
    "\n",
    "# Data creation - get context around the target word\n",
    "data = []\n",
    "tokens = test_vocab.tokens\n",
    "for i in range(CONTEXT_SIZE, len(tokens) - CONTEXT_SIZE):\n",
    "    # Context before w_i\n",
    "    context_before_w = tokens[i - CONTEXT_SIZE: i]\n",
    "    \n",
    "    # Context after w_i\n",
    "    context_after_w = tokens[i + 1: i + CONTEXT_SIZE + 1]\n",
    "    \n",
    "    # Put them together\n",
    "    context_window = context_before_w + context_after_w\n",
    "    \n",
    "    # Target = w_i\n",
    "    target = tokens[i]\n",
    "    \n",
    "    # Append in the correct format\n",
    "    data.append((context_window, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        #self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.vectorizer = \n",
    "        \n",
    "        # note: this probably doesn't deal with 'UNK' words\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons)  \n",
    "        \n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM)\n",
    "        \n",
    "        # NOTE: Leave these two commented out to ignore the embedding layer.\n",
    "        #embeds = self.embeddings(inputs)\n",
    "        #print(embeds.shape)\n",
    "    \n",
    "        # sum over all of the context vectors\n",
    "        # shape = (EMBEDDING_DIM)\n",
    "        summed_embeds = sum(inputs)\n",
    "\n",
    "        # shape = (1, EMBEDDING_DIM)\n",
    "        # -1 param in view() ... \"the actual value for this dimension will be inferred so that the number of elements in the view matches the original number of elements.\"\n",
    "        embeds_2D = summed_embeds.view(1, -1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds_2D) / self.context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h) \n",
    "         \n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding of 'that' before training:   tensor([0.9650, 0.7400, 0.3872, 0.1199, 0.6086, 0.6207, 0.2412, 0.3062, 0.1795,\n",
      "        0.4194, 0.2258, 0.2382, 0.6560, 0.5999, 0.4702, 0.0732, 0.3296, 0.0682,\n",
      "        0.8022, 0.7873, 0.1327, 0.0778, 0.9116, 0.1083, 0.5873, 0.8716, 0.2712,\n",
      "        0.5834, 0.3283, 0.6758, 0.2405, 0.5274, 0.8777, 0.0872, 0.6084, 0.5113,\n",
      "        0.1510, 0.8490, 0.3829, 0.7425, 0.2765, 0.9401, 0.5344, 0.7088, 0.1095,\n",
      "        0.8702, 0.1212, 0.3343, 0.7172, 0.2539], requires_grad=True)\n",
      "tensor([[0.4926, 0.2976, 0.3939, 0.2974, 0.5297, 0.8822, 0.8808, 0.6060, 0.5823,\n",
      "         0.4073, 0.2642, 0.5397, 0.8877, 0.6874, 0.9971, 0.8707, 0.4805, 0.6828,\n",
      "         0.9935, 0.7222, 0.7816, 0.5579, 0.9192, 0.6729, 0.8824, 0.4710, 0.9389,\n",
      "         0.3137, 0.3883, 0.4914, 0.6406, 0.1147, 0.2619, 0.3827, 0.3787, 0.5994,\n",
      "         0.5085, 0.0136, 0.8231, 0.3169, 0.3505, 0.5673, 0.4568, 0.9015, 0.9662,\n",
      "         0.4348, 0.4837, 0.6173, 0.6467, 0.9712],\n",
      "        [0.2934, 0.3514, 0.0341, 0.5294, 0.6085, 0.6365, 0.3236, 0.8200, 0.4276,\n",
      "         0.0748, 0.5063, 0.2981, 0.4795, 0.6364, 0.5253, 0.7889, 0.8924, 0.2690,\n",
      "         0.7909, 0.6431, 0.0773, 0.2397, 0.5343, 0.5778, 0.7112, 0.6973, 0.9985,\n",
      "         0.5947, 0.6010, 0.3115, 0.6691, 0.5076, 0.2833, 0.9271, 0.6275, 0.0912,\n",
      "         0.8126, 0.8966, 0.6381, 0.8323, 0.5110, 0.2174, 0.6161, 0.8620, 0.6972,\n",
      "         0.0780, 0.2436, 0.1453, 0.2724, 0.8772],\n",
      "        [0.9746, 0.3751, 0.9037, 0.4322, 0.0048, 0.6756, 0.8996, 0.9543, 0.4296,\n",
      "         0.5722, 0.5182, 0.0168, 0.0468, 0.0322, 0.6494, 0.6092, 0.0834, 0.7367,\n",
      "         0.9695, 0.3752, 0.8908, 0.2351, 0.9136, 0.6255, 0.1232, 0.3778, 0.7662,\n",
      "         0.6247, 0.8357, 0.3211, 0.8149, 0.2000, 0.5546, 0.1023, 0.6093, 0.5132,\n",
      "         0.7078, 0.6899, 0.8092, 0.2014, 0.9700, 0.4650, 0.0544, 0.8542, 0.3470,\n",
      "         0.9885, 0.3294, 0.3519, 0.6510, 0.9219],\n",
      "        [0.0823, 0.0785, 0.1754, 0.9866, 0.4929, 0.1721, 0.4611, 0.8597, 0.4243,\n",
      "         0.0909, 0.5444, 0.2376, 0.6624, 0.1308, 0.1809, 0.7376, 0.6949, 0.1020,\n",
      "         0.3557, 0.7720, 0.4771, 0.4078, 0.2591, 0.4893, 0.4834, 0.6252, 0.7683,\n",
      "         0.3176, 0.1780, 0.1545, 0.0478, 0.1680, 0.8094, 0.5619, 0.4350, 0.4995,\n",
      "         0.5759, 0.3671, 0.3046, 0.2345, 0.7654, 0.6357, 0.3468, 0.5677, 0.4896,\n",
      "         0.5034, 0.5417, 0.7294, 0.6766, 0.9125]], grad_fn=<StackBackward>) True\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss() # negative log likelihood loss\n",
    "model = CBOW(test_vocab.nr_unique_tokens, 50, CONTEXT_WINDOW_SIZE)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"embedding of 'that' before training:  \", vectorizer.vectors['that'])\n",
    "\n",
    "for epoch in range(NUM_ITERATIONS):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        # Step1. Create input vector \n",
    "        context_vectors = vectorizer.make_context_vector(context)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_vectors)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        target = torch.tensor(vectorizer.vocab.tok_to_ids[target], dtype=torch.long).view(1)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        #loss = loss_function(log_probs, torch.tensor(word_to_vec.vectors[target], dtype=torch.long))\n",
    "\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print('-' * 100)\n",
    "print(\"embedding of 'that' after training:  \", vectorizer.vectors['that'])\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_two = torch.tensor([[7,8,9],[10,11,12]])\n",
    "tensor_tre = torch.tensor([[13,14,15],[16,17,18]])\n",
    "tensor_list = [tensor_two, tensor_tre]\n",
    "mytensor = torch.stack(tensor_list)\n",
    "mytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of word_embeddings_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
