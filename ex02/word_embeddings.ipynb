{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DQTQQGF_TMcD",
    "outputId": "860d0c42-0384-4202-ef8b-b8370ef7d46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b93c730>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, add_unk=True):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        \n",
    "        self._token_to_ids = {}\n",
    "        self._ids_to_token = {}\n",
    "        \n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(\"<UNK>\") \n",
    "\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = list(set(self.tokens))\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_ids:\n",
    "            index = self._token_to_ids[token]\n",
    "        else:\n",
    "            index = len(self._token_to_ids)\n",
    "            self._token_to_ids[token] = index\n",
    "            self._ids_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_ids.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_ids[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._ids_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._ids_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "        \"\"\"\n",
    "        vocabulary = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            # add each context word (token) to the vocabulary\n",
    "            for token in row.context:\n",
    "                vocabulary.add_token(token)\n",
    "                \n",
    "            # add the target word as well\n",
    "            vocabulary.add_token(row.target)\n",
    "            \n",
    "        return cls(vocabulary)\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.lookup_token(w) for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "        \"\"\"\n",
    "        random_state = 100\n",
    "        self.train_df = cbow_df.sample(frac=0.96, random_state=random_state)\n",
    "        \n",
    "        self.val_df = cbow_df.drop(self.train_df.index)\n",
    "        self.val_df = self.val_df.sample(frac=0.5, random_state=random_state)\n",
    "        \n",
    "        self.test_df = self.val_df.drop(self.val_df.index)\n",
    "\n",
    "        self._lookup_dict = {'train': self.train_df,\n",
    "                             'val': self.val_df,\n",
    "                             'test': self.test_df}\n",
    "\n",
    "        self.set_split()\n",
    "        \n",
    "        self._vectorizer = Vectorizer.from_dataframe(self.train_df)\n",
    "\n",
    "    @classmethod\n",
    "    def load_and_create_dataset(cls, filepath, context_size):\n",
    "        \"\"\"Load and preprocess the dataset\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ShakespeareDataset\n",
    "        \"\"\"\n",
    "        lines = ShakespeareDataset.load_file(filepath)\n",
    "        \n",
    "        # Preprocess\n",
    "        tokens = ShakespeareDataset.preprocess_and_split_lines(lines)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        dataframe_data = ShakespeareDataset.create_context_data(tokens, context_size)\n",
    "        cbow_df = pd.DataFrame(dataframe_data, columns=['context', 'target'])\n",
    "        \n",
    "        # Create an instance \n",
    "        return cls(cbow_df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_file(filepath):\n",
    "        \"\"\"Load the dataset file into lines\"\"\"\n",
    "        with open(filepath) as file:\n",
    "            lines = file.readlines()\n",
    "            file.close()\n",
    "            return lines\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_and_split_lines(lines):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            lines (list): a list of lines of the dataset\n",
    "        Returns:\n",
    "            a list of tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        # Regex\n",
    "        lines = lines[134:164924] #these numbers are only valid for the full corpus\n",
    "        text = ''.join(lines)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_context_data(tokens, context_size):\n",
    "        data = []\n",
    "        for i in range(context_size, len(tokens) - context_size):\n",
    "            # Context before w_i\n",
    "            context_before_w = tokens[i - context_size: i]\n",
    "\n",
    "            # Context after w_i\n",
    "            context_after_w = tokens[i + 1: i + context_size + 1]\n",
    "\n",
    "            # Put them together\n",
    "            context_window = context_before_w + context_after_w\n",
    "\n",
    "            # Target = w_i\n",
    "            target = tokens[i]\n",
    "\n",
    "            # Append in the correct format\n",
    "            data.append([context_window, target])\n",
    "        return data\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_df = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = self._vectorizer.vectorize(row.context)\n",
    "        target_index = self._vectorizer.vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self._context_window_size = context_window_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # note: this probably doesn't deal with 'UNK' words\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons)  \n",
    "        \n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = self.embeddings(inputs).sum(dim=1)\n",
    "\n",
    "        # shape = (1, EMBEDDING_DIM)\n",
    "        # -1 param in view() ... \"the actual value for this dimension will be inferred so that the number of elements in the view matches the original number of elements.\"\n",
    "        #embeds_2D = embeds.view(1, -1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds) / self._context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h)\n",
    "         \n",
    "        # output\n",
    "        # also note that we don't compute softmax here because Cross Entropy is used as a loss function\n",
    "        out = F.relu(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    #shakespeare_csv_filepath=\"test_corpus.txt\",\n",
    "    shakespeare_csv_filepath=\"shakespeare-corpus.txt\",\n",
    "    # model_state_file=\"model.pth\",\n",
    "    # Model hyper parameters\n",
    "    context_size=2,\n",
    "    num_neurons=128,\n",
    "    embedding_dim=50,\n",
    "    # Training hyper parameters\n",
    "    seed=13337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    # Runtime options\n",
    "    cuda=True\n",
    ")\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "# Dataset\n",
    "dataset = ShakespeareDataset.load_and_create_dataset(\n",
    "    args.shakespeare_csv_filepath,\n",
    "    args.context_size\n",
    ")\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Classifier\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "classifier = CBOW(len(vectorizer.vocab), args.embedding_dim, args.context_size * 2, args.num_neurons)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch_index in tqdm(range(args.num_epochs)):\n",
    "    # train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # Iterate over training dataset\n",
    "\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    # train_state['train_loss'].append(running_loss)\n",
    "    # train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over val dataset\n",
    "\n",
    "    # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        # compute the output\n",
    "        y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    # train_state['val_loss'].append(running_loss)\n",
    "    # train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    # train_state = update_train_state(args=args, model=classifier,\n",
    "    #                                 train_state=train_state)\n",
    "\n",
    "\n",
    "    #if train_state['stop_early']:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part2 supplied function\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = classifier.embeddings\n",
    "    test_vocab = dataset.get_vectorizer().vocab\n",
    "    pdist = nn.CosineSimilarity()\n",
    "    i = test_vocab.lookup_token(word)\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long) \n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.lookup_index(j), float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1]) \n",
    "    return word_distance[:topn]\n",
    "\n",
    "get_closest_word('desire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_examples = len(data)\n",
    "pred_sum = 0 # softmax check\n",
    "acc_sum = 0 # accuracy\n",
    "\n",
    "for i in range(nr_examples):\n",
    "    ids = vectorizer.vectorize(data[i][0])\n",
    "    target = test_vocab.tok_to_ids[data[i][1]]\n",
    "    pred = model(ids) # prediction\n",
    "    pred_sum += pred.squeeze().sum().item() \n",
    "    \n",
    "    _, pred_index = pred.max(dim=1) # prediction index\n",
    "    n_correct = torch.eq(pred_index, target)\n",
    "    acc_sum += n_correct.item()\n",
    "    \n",
    "    print(\"Prediction: \" + str(pred_index.item()), \"| Target: \" + str(target))\n",
    "    \n",
    "print(acc_sum / nr_examples)\n",
    "print(pred_sum / nr_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringo = \"here is an [_exit_]\"\n",
    "stringo = re.sub('(\\[_).*(_\\])', '', stringo)\n",
    "print(stringo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finis is 164924\n",
    "#beginngin is line 134 --> just keep what's in between those lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'shakespeare-corpus.txt'\n",
    "file = open(filename)\n",
    "lines = file.readlines()\n",
    "lines = lines[134:164924]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytext(lines):\n",
    "    corpus = ''\n",
    "    for line in lines:\n",
    "        text = re.sub(r'\\d+', '', line)\n",
    "        text = re.sub('SCENE \\S', '', text)\n",
    "        text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "        text = text.lower()\n",
    "        corpus += text\n",
    "    return corpus\n",
    "\n",
    "%time len(mytext(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytext2(lines):\n",
    "    text = ''.join(lines)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('SCENE \\S', '', text)\n",
    "    text = re.sub('(\\[_).*(_\\])', '', text)\n",
    "    text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+_', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "%time len(mytext2(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of word_embeddings_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
