{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DQTQQGF_TMcD",
    "outputId": "860d0c42-0384-4202-ef8b-b8370ef7d46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11443a350>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "* http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf\n",
    "* https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\n",
    "* https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py\n",
    "* https://stackoverflow.com/questions/50792316/what-does-1-mean-in-pytorch-view\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "* https://github.com/ChristophAlt/embedding_vectorizer/blob/master/embedding_vectorizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "class Vocabulary():\n",
    "    def __init__(self, filepath):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.tokens = self.nltk_tokenize()\n",
    "        self.tok_to_ids, self.ids_to_tok = self.make_dicts()\n",
    "    \n",
    "    def readfile(self):\n",
    "        \"\"\"this function opens the file and returns the text in a string\"\"\"\n",
    "        file = open(self.filepath)\n",
    "        text = file.read()\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = \n",
    "        text = re.sub(r'[\\\\[#$%*+—/<=>?{}|~@]+', '', text)\n",
    "        text = text.lower()\n",
    "        file.close()\n",
    "        return text\n",
    "    \n",
    "    def nltk_tokenize(self):\n",
    "        \"\"\"this function tokenizes the text and returns a list of tokens as strings\"\"\"\n",
    "        text = self.readfile()\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def vocabulary_set(self):\n",
    "        \"\"\"this function returns a list of unique tokens\"\"\"\n",
    "        return(list(set(self.tokens)))\n",
    "    \n",
    "    def make_dicts(self):\n",
    "        unique_tokens = list(set(self.tokens))\n",
    "        tok_to_ix = {}\n",
    "        ix_to_tok = {}\n",
    "        for i in range(len(unique_tokens)):\n",
    "            tok_to_ix.update({unique_tokens[i]: i})\n",
    "            ix_to_tok.update({i: unique_tokens[i]})\n",
    "        return tok_to_ix, ix_to_tok\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tok_to_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "    \n",
    "    def vectorize(self, context_words):\n",
    "        context_ids = [self.vocab.tok_to_ids[w] for w in context_words]\n",
    "        return torch.tensor(context_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (Vectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "        \n",
    "        self.train_df = self.cbow_df[self.cbow_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split=='train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'test_corpus.txt'\n",
    "test_vocab = Vocabulary(filepath)\n",
    "vectorizer = Vectorizer(test_vocab)\n",
    "\n",
    "# Size of the context windows, 2 and 5 are supposed to be used in ex02...\n",
    "# range \\in [2, 1/2 * document_length - 1]\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# let's stick with this notation for now ;)\n",
    "CONTEXT_WINDOW_SIZE = CONTEXT_SIZE * 2\n",
    "\n",
    "\n",
    "# Data creation - get context around the target word\n",
    "data = []\n",
    "tokens = test_vocab.tokens\n",
    "for i in range(CONTEXT_SIZE, len(tokens) - CONTEXT_SIZE):\n",
    "    # Context before w_i\n",
    "    context_before_w = tokens[i - CONTEXT_SIZE: i]\n",
    "    \n",
    "    # Context after w_i\n",
    "    context_after_w = tokens[i + 1: i + CONTEXT_SIZE + 1]\n",
    "    \n",
    "    # Put them together\n",
    "    context_window = context_before_w + context_after_w\n",
    "    \n",
    "    # Target = w_i\n",
    "    target = tokens[i]\n",
    "    \n",
    "    # Append in the correct format\n",
    "    data.append((context_window, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, nr_hidden_neurons=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # note: this probably doesn't deal with 'UNK' words\n",
    "        self.linear1 = nn.Linear(embedding_dim, nr_hidden_neurons)  \n",
    "        \n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(nr_hidden_neurons, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # shape = (WINDOW_SIZE, EMBEDDING_DIM) -> (EMBEDDING_DIM)\n",
    "        embeds = sum(self.embeddings(inputs))\n",
    "\n",
    "        # shape = (1, EMBEDDING_DIM)\n",
    "        # -1 param in view() ... \"the actual value for this dimension will be inferred so that the number of elements in the view matches the original number of elements.\"\n",
    "        embeds_2D = embeds.view(1, -1)\n",
    "        \n",
    "        # finally compute the hidden layer weighted sum (a.k.a. output before using the activation function)\n",
    "        # ... and don't forget to divide by the number of input vectors\n",
    "        h =  self.linear1(embeds_2D) / self.context_window_size\n",
    "        \n",
    "        # output of the hidden layer\n",
    "        out =  F.relu(h) \n",
    "         \n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.softmax(out, dim=-1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2968, -1.3363, -0.6327,  ...,  1.2514, -0.7788,  0.3514],\n",
      "        [ 0.0991,  1.3557,  1.0691,  ..., -1.1648,  1.9188,  0.1217],\n",
      "        [ 1.2967, -0.9874,  2.6904,  ...,  0.8510,  1.4434, -0.6353],\n",
      "        ...,\n",
      "        [-1.2968,  1.1442,  0.6030,  ..., -2.8753,  1.1613, -0.9076],\n",
      "        [ 1.9019,  0.1653, -0.9064,  ..., -0.2193, -0.7089, -1.8231],\n",
      "        [ 1.0605,  0.7648,  0.6156,  ..., -0.9671,  0.9416, -0.6220]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/100 [00:00<00:37,  2.63it/s]\u001b[A\n",
      "  2%|▏         | 2/100 [00:00<00:37,  2.61it/s]\u001b[A\n",
      "  3%|▎         | 3/100 [00:01<00:36,  2.63it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:01<00:37,  2.54it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:01<00:37,  2.52it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:02<00:37,  2.50it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:02<00:37,  2.49it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:03<00:36,  2.52it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:03<00:36,  2.52it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:03<00:34,  2.59it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:04<00:33,  2.64it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:04<00:33,  2.64it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:05<00:32,  2.69it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:05<00:32,  2.68it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:05<00:32,  2.62it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:06<00:31,  2.65it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:06<00:31,  2.66it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:06<00:30,  2.68it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:07<00:29,  2.72it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:07<00:29,  2.69it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:08<00:28,  2.73it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:08<00:29,  2.64it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:08<00:30,  2.55it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:09<00:30,  2.52it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:09<00:31,  2.42it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:10<00:30,  2.42it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:10<00:30,  2.37it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:11<00:31,  2.30it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:11<00:31,  2.25it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:11<00:31,  2.21it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:12<00:32,  2.14it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:13<00:34,  1.97it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:13<00:36,  1.83it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:14<00:36,  1.82it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:14<00:36,  1.80it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:15<00:35,  1.80it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:15<00:35,  1.75it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:16<00:37,  1.68it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:17<00:37,  1.61it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:17<00:38,  1.56it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:18<00:37,  1.58it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:19<00:36,  1.59it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:19<00:36,  1.56it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:20<00:39,  1.41it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:21<00:42,  1.29it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:22<00:38,  1.40it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:22<00:35,  1.48it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:23<00:33,  1.56it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:24<00:32,  1.59it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:24<00:30,  1.64it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:25<00:29,  1.67it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:25<00:28,  1.67it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:26<00:27,  1.70it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:26<00:26,  1.71it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:27<00:25,  1.75it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:28<00:25,  1.75it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:28<00:24,  1.75it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:29<00:23,  1.76it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:29<00:23,  1.74it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:30<00:22,  1.79it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:30<00:22,  1.76it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:31<00:22,  1.71it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:32<00:24,  1.52it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:32<00:23,  1.51it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:33<00:22,  1.58it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:34<00:20,  1.64it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:34<00:19,  1.67it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:35<00:18,  1.72it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:35<00:18,  1.71it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:36<00:16,  1.78it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:36<00:16,  1.81it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:37<00:15,  1.86it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:37<00:14,  1.89it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:38<00:13,  1.91it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:38<00:13,  1.86it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:39<00:12,  1.88it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:39<00:12,  1.87it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:40<00:11,  1.92it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:41<00:11,  1.89it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:41<00:10,  1.89it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:42<00:09,  1.92it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:42<00:09,  1.95it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:43<00:08,  1.95it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:43<00:08,  1.96it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:44<00:07,  1.97it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:44<00:07,  1.92it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:45<00:06,  1.89it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:45<00:06,  1.86it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:46<00:05,  1.87it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:46<00:05,  1.84it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:47<00:04,  1.86it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:47<00:04,  1.85it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:48<00:03,  1.89it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:49<00:03,  1.67it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:49<00:03,  1.59it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:50<00:02,  1.67it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:50<00:01,  1.73it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:51<00:01,  1.79it/s]\u001b[A\n",
      " 99%|█████████▉| 99/100 [00:51<00:00,  1.81it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:52<00:00,  1.87it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2157.2932534217834, 2149.621421813965, 2146.9821467399597, 2139.949531555176, 2144.2452030181885, 2136.0354237556458, 2132.1979837417603, 2127.3439860343933, 2128.0021467208862, 2125.1851954460144, 2127.101399421692, 2130.385423183441, 2121.887942790985, 2118.178158760071, 2118.3816537857056, 2117.4009971618652, 2116.4041271209717, 2120.2487359046936, 2113.8172221183777, 2112.163456439972, 2115.0908341407776, 2114.348201274872, 2117.360589981079, 2117.4578924179077, 2115.4352502822876, 2115.4409980773926, 2115.4506645202637, 2115.444701194763, 2115.4482140541077, 2117.388551712036, 2118.4399399757385, 2118.4498629570007, 2118.451989173889, 2118.447032928467, 2117.4498982429504, 2127.0068163871765, 2122.151915550232, 2120.648383617401, 2117.672125816345, 2113.792321205139, 2119.4125757217407, 2117.4586849212646, 2117.4580750465393, 2118.2634258270264, 2116.452977657318, 2115.458373069763, 2115.460765838623, 2116.024847984314, 2116.356249332428, 2113.4573369026184, 2114.4621086120605, 2114.459030151367, 2114.4629163742065, 2114.461564064026, 2114.46235704422, 2114.462956428528, 2114.4658188819885, 2114.4623436927795, 2116.008114337921, 2115.4544801712036, 2114.5722517967224, 2116.4541325569153, 2118.467118740082, 2120.453664302826, 2120.457962036133, 2117.9692764282227, 2112.192358493805, 2112.4528579711914, 2112.4502935409546, 2112.45840883255, 2113.460735797882, 2113.460551261902, 2115.460307121277, 2117.4581422805786, 2114.463040828705, 2114.4564294815063, 2113.460406780243, 2112.45361995697, 2117.420334815979, 2113.7107853889465, 2116.1883363723755, 2115.449279308319, 2120.2147789001465, 2116.474997997284, 2116.4654397964478, 2115.4554471969604, 2115.4567580223083, 2119.4820985794067, 2113.4571261405945, 2113.458830833435, 2113.461087703705, 2113.455728530884, 2113.4623279571533, 2112.46040058136, 2112.4735140800476, 2112.4608674049377, 2112.458860397339, 2113.4612336158752, 2112.4573335647583, 2115.458372116089]\n",
      "Parameter containing:\n",
      "tensor([[ 0.3892, -1.6136,  0.6729,  ...,  1.5775,  0.5987,  0.5898],\n",
      "        [ 0.1082,  0.6969,  0.3804,  ..., -1.6798,  2.8077, -0.6952],\n",
      "        [ 1.6968,  0.4015,  3.6844,  ..., -0.9036, -0.5268, -0.1337],\n",
      "        ...,\n",
      "        [-2.4887, -0.4898,  2.2391,  ..., -2.0599, -0.9341,  0.7384],\n",
      "        [ 2.2251,  0.4671, -1.1238,  ...,  0.0862,  0.8877, -1.5826],\n",
      "        [ 0.9522,  0.6300, -0.3345,  ..., -0.8952,  1.3729, -1.1666]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "NUM_ITERATIONS = 100\n",
    "NUM_NEURONS = 100\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = CBOW(len(test_vocab), EMBEDDING_DIM, CONTEXT_WINDOW_SIZE, NUM_NEURONS)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model.embeddings.weight)\n",
    "\n",
    "for epoch in tqdm(range(NUM_ITERATIONS)):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        # Step1. Create input vector \n",
    "        context_vector_ids = vectorizer.vectorize(context)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        softmax = model(context_vector_ids)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        target = torch.tensor(vectorizer.vocab.tok_to_ids[target], dtype=torch.long).view(1)\n",
    "        loss = loss_function(softmax, target)\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "print(losses)\n",
    "print(model.embeddings.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## OOP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shakespeare_csv_filepath = 'test_corpus.txt'\n",
    "#dataset = ShakespeareDataset.load_dataset_and_make_vectorizer(shakespeare_csv_filepath)\n",
    "#dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "#vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "#classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab), embedding_size=args.embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Test your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die', 7.657770156860352),\n",
       " ('How', 9.359249114990234),\n",
       " ('Were', 9.374570846557617),\n",
       " ('only', 9.418946266174316),\n",
       " ('say', 9.439167976379395)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part2 supplied function\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = model.embeddings\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = test_vocab.tok_to_ids[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long) \n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(test_vocab)): \n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j) \n",
    "            word_distance.append((test_vocab.ids_to_tok[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1]) \n",
    "    return word_distance[:topn]\n",
    "\n",
    "get_closest_word('that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1882640586797066\n",
      "0.9999999992713369\n"
     ]
    }
   ],
   "source": [
    "nr_examples = len(data)\n",
    "pred_sum = 0 # softmax check\n",
    "acc_sum = 0 # accuracy\n",
    "\n",
    "for i in range(nr_examples):\n",
    "    ids = vectorizer.vectorize(data[i][0])\n",
    "    target = test_vocab.tok_to_ids[data[i][1]]\n",
    "    pred = model(ids) # prediction\n",
    "    pred_sum += pred.squeeze().sum().item() \n",
    "    \n",
    "    _, pred_indices = pred.max(dim=1) # prediction index\n",
    "    n_correct = torch.eq(pred_indices, target)\n",
    "    acc_sum += n_correct.item()\n",
    "    \n",
    "print(acc_sum / nr_examples)\n",
    "print(pred_sum / nr_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is an [_exit_]\n"
     ]
    }
   ],
   "source": [
    "stringo = \"here is an [_exit_]\"\n",
    "stringo = re.sub('\\\\[_*_\\]', '', stringo)\n",
    "print(stringo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finis is 164924\n",
    "#beginngin is line 134 --> just keep what's in between those lines\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of word_embeddings_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
